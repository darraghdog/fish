{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in Libraries\n",
    "from __future__ import division, print_function\n",
    "from logbook import Logger, StreamHandler\n",
    "import sys\n",
    "StreamHandler(sys.stdout).push_application()\n",
    "log = Logger('Logbook')\n",
    "import shutil, csv, time\n",
    "timestr = time.strftime(\"%Y%m%d\")\n",
    "import ujson as json\n",
    "import utils; reload(utils)\n",
    "from utils import *\n",
    "import gc\n",
    "# from __future__ import division, print_function\n",
    "from theano.sandbox import cuda\n",
    "from vgg16bn import Vgg16BN\n",
    "from sklearn import metrics\n",
    "\n",
    "def accuracyfunc(y_act, y_pred):\n",
    "    return metrics.accuracy_score(np.argmax(y_act, axis=1), np.argmax(y_pred, axis=1))\n",
    "    \n",
    "def refresh_directory_structure(name, sub_dirs):\n",
    "    gdir = os.path.join(path, name)\n",
    "    if os.path.exists(gdir):\n",
    "        shutil.rmtree(gdir)\n",
    "    os.makedirs(gdir)\n",
    "    for sub_dir in sub_dirs:\n",
    "        os.makedirs(os.path.join(gdir, sub_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-02-17 23:10:23.710057] INFO: Logbook: Set Paramters\n"
     ]
    }
   ],
   "source": [
    "# Set Parameters and check files\n",
    "refresh_directories = False\n",
    "input_exists = True\n",
    "full = False\n",
    "log.info('Set Paramters')\n",
    "path = \"../data/fish/\"\n",
    "batch_size=64\n",
    "clip = 0.99\n",
    "bags = 1\n",
    "load_size = (360, 640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the test and valid directory\n",
    "if refresh_directories:\n",
    "    log.info('Create directory structure and validation files')\n",
    "    sub_dirs = os.listdir(os.path.join(path, 'train-all'))\n",
    "    if '.DS_Store' in sub_dirs: sub_dirs.remove('.DS_Store')\n",
    "    refresh_directory_structure('train', sub_dirs)\n",
    "    refresh_directory_structure('valid', sub_dirs)\n",
    "    for c,row in enumerate(csv.DictReader(open('../image_validation_set.csv'))):\n",
    "        value = 'valid' if row['Validation'] == '1' else 'train'\n",
    "        name_from = os.path.join(path, 'train-all', row['SubDirectory'], row['file_name'])\n",
    "        name_to   = os.path.join(path, value, row['SubDirectory'], row['file_name'])\n",
    "        shutil.copyfile(name_from, name_to)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-02-17 23:10:23.734193] INFO: Logbook: Get VGG\n",
      "[2017-02-17 23:10:26.798368] INFO: Logbook: Create VGG\n",
      "Found 3086 images belonging to 8 classes.\n",
      "Found 691 images belonging to 8 classes.\n",
      "Found 1000 images belonging to 1 classes.\n",
      "[2017-02-17 23:10:27.319593] INFO: Logbook: Read filenames\n"
     ]
    }
   ],
   "source": [
    "# Read in our VGG pretrained model\n",
    "log.info('Get VGG')\n",
    "model = vgg_ft_bn(8)\n",
    "\n",
    "# Create our VGG model\n",
    "log.info('Create VGG')\n",
    "vgg640 = Vgg16BN(load_size).model\n",
    "vgg640.pop()\n",
    "vgg640.input_shape, vgg640.output_shape\n",
    "vgg640.compile(Adam(), 'categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# get labels\n",
    "(val_classes, trn_classes, val_labels, trn_labels,\n",
    "    val_filenames, filenames, test_filenames) = get_classes(path)\n",
    "\n",
    "# Read in filenames\n",
    "log.info('Read filenames')\n",
    "raw_filenames = [f.split('/')[-1] for f in filenames]\n",
    "raw_test_filenames = [f.split('/')[-1] for f in test_filenames]\n",
    "raw_val_filenames = [f.split('/')[-1] for f in val_filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in the boxes\n",
    "anno_classes = ['alb', 'bet', 'dol', 'lag', 'other', 'shark', 'yft']\n",
    "bb_json = {}\n",
    "for c in anno_classes:\n",
    "    j = json.load(open(os.path.join(path, 'box/{}_labels.json'.format(c)), 'r'))\n",
    "    for l in j:\n",
    "        if 'annotations' in l.keys() and len(l['annotations'])>0:\n",
    "            bb_json[l['filename'].split('/')[-1]] = sorted(\n",
    "                l['annotations'], key=lambda x: x['height']*x['width'])[-1]\n",
    "\n",
    "# make it easy to find the nof dots, by putting themin the middle\n",
    "empty_bbox = {'height': 0., 'width': 0., 'x': 1280/2., 'y': 720/2}\n",
    "\n",
    "for f in raw_filenames:\n",
    "    if not f in bb_json.keys(): bb_json[f] = empty_bbox\n",
    "for f in raw_val_filenames:\n",
    "    if not f in bb_json.keys(): bb_json[f] = empty_bbox\n",
    "\n",
    "# Finally, we convert the dictionary into an array, and convert the coordinates to our resized 224x224 images.\n",
    "bb_params = ['height', 'width', 'x', 'y']\n",
    "def convert_bb(bb, size):\n",
    "    bb = [bb[p] for p in bb_params]\n",
    "    conv_x = (224. / size[0])\n",
    "    conv_y = (224. / size[1])\n",
    "    bb[0] = bb[0]*conv_y\n",
    "    bb[1] = bb[1]*conv_x\n",
    "    bb[2] = max(bb[2]*conv_x, 0)\n",
    "    bb[3] = max(bb[3]*conv_y, 0)\n",
    "    return bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trn_sizes = [PIL.Image.open(path+'train/'+f).size for f in filenames]\n",
    "val_sizes = [PIL.Image.open(path+'valid/'+f).size for f in val_filenames]\n",
    "tst_sizes = [PIL.Image.open(path+'test/'+f).size for f in test_filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trn_bbox = np.stack([convert_bb(bb_json[f], s) for f,s in zip(raw_filenames, trn_sizes)], \n",
    "                   ).astype(np.float32)\n",
    "val_bbox = np.stack([convert_bb(bb_json[f], s) \n",
    "                   for f,s in zip(raw_val_filenames, val_sizes)]).astype(np.float32)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 691 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "                      \n",
    "\n",
    "def create_rect(bb, color='red'):\n",
    "    return plt.Rectangle((bb[2], bb[3]), bb[1], bb[0], color=color, fill=False, lw=3)\n",
    "\n",
    "def show_bb(i):\n",
    "    bb = val_bbox[i]\n",
    "    img  = np.rollaxis(val[i], 0, 3).astype(np.uint8)\n",
    "    plt.scatter(bb[0], bb[1], color='red', s = 100)\n",
    "    plt.imshow(img)\n",
    "    \n",
    "val = get_data(path+'valid', load_size)\n",
    "show_bb(98)\n",
    "# check whats going wrong here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-02-17 23:11:05.705796] INFO: Logbook: Read in data\n"
     ]
    }
   ],
   "source": [
    "log.info('Read in data')\n",
    "if not input_exists:\n",
    "\n",
    "    batches = get_batches(path+'train', batch_size=batch_size)\n",
    "    val_batches = get_batches(path+'valid', batch_size=batch_size*2, shuffle=False)\n",
    "    (val_classes, trn_classes, val_labels, trn_labels, \n",
    "        val_filenames, filenames, test_filenames) = get_classes(path)\n",
    "    \n",
    "    # Fetch our large images \n",
    "    log.info('Fetch images')\n",
    "    trn = get_data(path+'train', load_size)\n",
    "    val = get_data(path+'valid', load_size)\n",
    "    test = get_data(path+'test', load_size)\n",
    "    \n",
    "    # Precompute the output of the convolutional part of VGG\n",
    "    log.info('Get VGG output')\n",
    "    conv_val_feat = vgg640.predict(val, batch_size=32, verbose=1)\n",
    "    conv_trn_feat = vgg640.predict(trn, batch_size=32, verbose=1)\n",
    "    conv_test_feat = vgg640.predict(test, batch_size=32, verbose=1)\n",
    "    log.info('Write VGG output')\n",
    "    save_array(path+'results/conv_val_feat.dat', conv_val_feat)\n",
    "    save_array(path+'results/conv_trn_feat.dat', conv_trn_feat) \n",
    "    save_array(path+'results/conv_test_feat.dat', conv_test_feat)     \n",
    "\n",
    "    # For memory purposes delete out the original train and validation\n",
    "    log.info('Clear up memory')\n",
    "    del trn, val, test\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_val_feat = load_array(path+'results/conv_val_feat.dat')\n",
    "conv_trn_feat = load_array(path+'results/conv_trn_feat.dat') \n",
    "conv_test_feat = load_array(path+'results/conv_test_feat.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-02-17 23:11:11.651988] INFO: Logbook: Create and fit CNN\n"
     ]
    }
   ],
   "source": [
    "if full:\n",
    "    conv_trn_feat = np.concatenate([conv_trn_feat, conv_val_feat])\n",
    "    trn_labels = np.concatenate([trn_labels, val_labels]) \n",
    "    \n",
    "# Our Convolutional Net Architecture\n",
    "log.info('Create and fit CNN')\n",
    "p=0.6\n",
    "# Set up the fully convolutional net (FCN); \n",
    "conv_layers,_ = split_at(vgg640, Convolution2D)\n",
    "nf=128; p=0. # No dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th\n",
      "Train on 3086 samples, validate on 691 samples\n",
      "Epoch 1/2\n",
      "3086/3086 [==============================] - 17s - loss: 29.8350 - bb_loss: 5774.6978 - class_loss: 0.9615 - bb_acc: 0.4802 - class_acc: 0.6870 - val_loss: 30.1015 - val_bb_loss: 5609.8759 - val_class_loss: 2.0521 - val_bb_acc: 0.6932 - val_class_acc: 0.4052\n",
      "Epoch 2/2\n",
      "3086/3086 [==============================] - 17s - loss: 28.3477 - bb_loss: 5610.5544 - class_loss: 0.2949 - bb_acc: 0.5625 - class_acc: 0.9362 - val_loss: 28.9406 - val_bb_loss: 5502.8782 - val_class_loss: 1.4263 - val_bb_acc: 0.6527 - val_class_acc: 0.5601\n",
      "Train on 3086 samples, validate on 691 samples\n",
      "Epoch 1/4\n",
      "3086/3086 [==============================] - 17s - loss: 27.5649 - bb_loss: 5490.1473 - class_loss: 0.1141 - bb_acc: 0.6011 - class_acc: 0.9848 - val_loss: 28.6867 - val_bb_loss: 5475.2200 - val_class_loss: 1.3106 - val_bb_acc: 0.6483 - val_class_acc: 0.5687\n",
      "Epoch 2/4\n",
      "3086/3086 [==============================] - 17s - loss: 26.9768 - bb_loss: 5383.2713 - class_loss: 0.0604 - bb_acc: 0.6244 - class_acc: 0.9958 - val_loss: 28.2522 - val_bb_loss: 5433.6819 - val_class_loss: 1.0837 - val_bb_acc: 0.6527 - val_class_acc: 0.6382\n",
      "Epoch 3/4\n",
      "3086/3086 [==============================] - 17s - loss: 26.5644 - bb_loss: 5304.6708 - class_loss: 0.0411 - bb_acc: 0.6303 - class_acc: 0.9994 - val_loss: 27.8804 - val_bb_loss: 5381.5587 - val_class_loss: 0.9726 - val_bb_acc: 0.7019 - val_class_acc: 0.6831\n",
      "Epoch 4/4\n",
      "3086/3086 [==============================] - 18s - loss: 26.1825 - bb_loss: 5230.9732 - class_loss: 0.0276 - bb_acc: 0.6406 - class_acc: 0.9994 - val_loss: 27.5133 - val_bb_loss: 5285.4957 - val_class_loss: 1.0858 - val_bb_acc: 0.6614 - val_class_acc: 0.6498\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f51477c5e50>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nf = 512\n",
    "p  = 0.6\n",
    "inp = Input(conv_layers[-1].output_shape[1:])\n",
    "x = MaxPooling2D()(inp)\n",
    "x = ZeroPadding2D((1,1))(x)\n",
    "x = Convolution2D(nf,3,3, activation='relu', border_mode='same')(x)\n",
    "x = BatchNormalization(axis=1)(x)\n",
    "x = MaxPooling2D()(x)\n",
    "x = ZeroPadding2D((1,1))(x)\n",
    "x = Convolution2D(nf,3,3, activation='relu', border_mode='same')(x)\n",
    "x = BatchNormalization(axis=1)(x)\n",
    "x = MaxPooling2D()(x)\n",
    "x = ZeroPadding2D((1,1))(x)\n",
    "x = Convolution2D(nf,3,3, activation='relu', border_mode='same')(x)\n",
    "x = BatchNormalization(axis=1)(x)\n",
    "x1 =  MaxPooling2D()(x)\n",
    "x1 =  Convolution2D(8,3,3, border_mode='same')(x1)\n",
    "x1 = GlobalAveragePooling2D()(x1)\n",
    "x = Dropout(p/4)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(p)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(p/2)(x)\n",
    "x_bb = Dense(4, name='bb')(x)\n",
    "x_class = Dense(8, activation='softmax', name='class')(x1)\n",
    "\n",
    "## Set up the fully convolutional net (FCN); \n",
    "#conv_layers,_ = split_at(vgg640, Convolution2D)\n",
    "#nf=128; p=0. # No dropout\n",
    "\n",
    "lrg_model = []\n",
    "predsls = []\n",
    "pvalsls = []\n",
    "\n",
    "model = Model([inp], [x_bb, x_class])\n",
    "#model.summary()\n",
    "model.compile(Adam(lr=0.0001), loss=['mse', 'categorical_crossentropy'], metrics=['accuracy'],\n",
    "             loss_weights=[.005, 1.])\n",
    "model.fit(conv_trn_feat, [trn_bbox, trn_labels], batch_size=batch_size, nb_epoch=2, \n",
    "             validation_data=(conv_val_feat, [val_bbox, val_labels]))\n",
    "model.optimizer.lr = 1e-6\n",
    "model.fit(conv_trn_feat, [trn_bbox, trn_labels], batch_size=batch_size, nb_epoch=4, \n",
    "             validation_data=(conv_val_feat, [val_bbox, val_labels]))\n",
    "#model.optimizer.lr = 1e-7\n",
    "#model.fit(conv_trn_feat, [trn_bbox, trn_labels], batch_size=batch_size, nb_epoch=10, \n",
    "#             validation_data=(conv_val_feat, [val_bbox, val_labels]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-02-17 23:10:11.624412] INFO: Logbook: Train round0\n",
      "th\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "batchnormalization_125 (BatchNorm(None, 512, 22, 40)   1024        batchnormalization_input_1[0][0] \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_137 (Convolution2D)(None, 128, 22, 40)   589952      batchnormalization_125[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_126 (BatchNorm(None, 128, 22, 40)   256         convolution2d_137[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_116 (MaxPooling2D)  (None, 128, 11, 20)   0           batchnormalization_126[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_138 (Convolution2D)(None, 128, 11, 20)   147584      maxpooling2d_116[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_127 (BatchNorm(None, 128, 11, 20)   256         convolution2d_138[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_117 (MaxPooling2D)  (None, 128, 5, 10)    0           batchnormalization_127[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_139 (Convolution2D)(None, 128, 5, 10)    147584      maxpooling2d_117[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_128 (BatchNorm(None, 128, 5, 10)    256         convolution2d_139[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_118 (MaxPooling2D)  (None, 128, 5, 5)     0           batchnormalization_128[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_140 (Convolution2D)(None, 8, 5, 5)       9224        maxpooling2d_118[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dropout_66 (Dropout)             (None, 8, 5, 5)       0           convolution2d_140[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "globalaveragepooling2d_26 (Global(None, 8)             0           dropout_66[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 8)             0           globalaveragepooling2d_26[0][0]  \n",
      "====================================================================================================\n",
      "Total params: 896136\n",
      "____________________________________________________________________________________________________\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Error when checking model target: expected activation_1 to have shape (None, 8) but got array with shape (3086, 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-2ab45a9713b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mlrg_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mse'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mse'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     lrg_model[i].fit(conv_trn_feat, trn_bbox, batch_size=batch_size, nb_epoch=2,\n\u001b[0;32m---> 19\u001b[0;31m                  validation_data=(conv_val_feat, val_bbox))             \n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mlrg_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     lrg_model[i].fit(conv_trn_feat, trn_bbox, batch_size=batch_size, nb_epoch=6,\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m                               \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m                               sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight)\u001b[0m\n\u001b[1;32m   1032\u001b[0m                                                            \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m                                                            \u001b[0mcheck_batch_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m                                                            batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1035\u001b[0m         \u001b[0;31m# prepare validation data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_dim, batch_size)\u001b[0m\n\u001b[1;32m    963\u001b[0m                                    \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m                                    \u001b[0mcheck_batch_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m                                    exception_prefix='model target')\n\u001b[0m\u001b[1;32m    966\u001b[0m         sample_weights = standardize_sample_weights(sample_weight,\n\u001b[1;32m    967\u001b[0m                                                     self.output_names)\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_dim, exception_prefix)\u001b[0m\n\u001b[1;32m    106\u001b[0m                                         \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                                         \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m                                         str(array.shape))\n\u001b[0m\u001b[1;32m    109\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Error when checking model target: expected activation_1 to have shape (None, 8) but got array with shape (3086, 4)"
     ]
    }
   ],
   "source": [
    "#########################################################\n",
    "####                                                 ####\n",
    "####                                                 ####\n",
    "####             Still to be updated                 ####\n",
    "####                                                 ####\n",
    "####                                                 ####\n",
    "#########################################################\n",
    "\n",
    "\n",
    "for i in range(bags):\n",
    "\n",
    "    log.info('Train round' + str(i))\n",
    "    lrg_model.append(Sequential(get_lrg_layers()))\n",
    "    if i == 0:\n",
    "        lrg_model[i].summary()\n",
    "    #lrg_model[i].compile(Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    lrg_model[i].compile(Adam(lr=0.0001), loss=['mse'], metrics=['mse'])\n",
    "    lrg_model[i].fit(conv_trn_feat, trn_bbox, batch_size=batch_size, nb_epoch=2,\n",
    "                 validation_data=(conv_val_feat, val_bbox))             \n",
    "    lrg_model[i].optimizer.lr=1e-7\n",
    "    lrg_model[i].fit(conv_trn_feat, trn_bbox, batch_size=batch_size, nb_epoch=6,\n",
    "                 validation_data=(conv_val_feat, val_bbox))\n",
    "\n",
    "    # Make our prediction on the lrg_model layer\n",
    "    log.info('Output Prediction')\n",
    "    predsls.append(lrg_model[i].predict(conv_test_feat, batch_size=batch_size)) # or try 32 batch_size\n",
    "    pvalsls.append(lrg_model[i].predict(conv_val_feat, batch_size=batch_size))\n",
    "    val_score = \"%.3f\" % metrics.log_loss(val_labels, sum(pvalsls)/len(pvalsls))\n",
    "    acc_score = \"%.3f\" % accuracyfunc(val_labels, do_clip(sum(pvalsls)/len(pvalsls), clip))\n",
    "    log.info('Bagged Validation Logloss ' + str(val_score))\n",
    "    log.info('Bagged Validation Accuracy ' + str(acc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# metrics.log_loss(val_labels, do_clip(sum(pvalsls)/len(pvalsls), .9999))\n",
    "preds = sum(predsls)/len(predsls)\n",
    "subm = do_clip(preds, clip)\n",
    "\n",
    "if full:\n",
    "    subm_name = path+'results/subm_full_conv_' + timestr + '.csv.gz'\n",
    "else:\n",
    "    subm_name = path+'results/subm_part_conv_' + timestr + '.csv.gz'\n",
    "\n",
    "classes = ['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT']\n",
    "submission = pd.DataFrame(subm, columns=classes)\n",
    "submission.insert(0, 'image', raw_test_filenames)\n",
    "submission.to_csv(subm_name, index=False, compression='gzip')\n",
    "log.info('Done - files @ ' + subm_name)\n",
    "\n",
    "# Bag 6 Original scores \n",
    "#[2017-02-09 22:40:05.864336] INFO: Logbook: Bagged Validation Logloss 1.046\n",
    "#[2017-02-09 22:40:05.864498] INFO: Logbook: Bagged Validation Accuracy 0.706"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
