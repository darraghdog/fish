{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-02-06 20:45:01.128384] INFO: Logbook: Set Paramters\n",
      "[2017-02-06 20:45:01.129851] INFO: Logbook: Get VGG\n",
      "[2017-02-06 20:45:04.556287] INFO: Logbook: Create VGG\n",
      "Found 3277 images belonging to 8 classes.\n",
      "Found 500 images belonging to 8 classes.\n",
      "Found 1000 images belonging to 1 classes.\n",
      "[2017-02-06 20:45:05.077589] INFO: Logbook: Read filenames\n"
     ]
    }
   ],
   "source": [
    "# Read in Libraries\n",
    "from __future__ import division, print_function\n",
    "from logbook import Logger, StreamHandler\n",
    "import sys\n",
    "StreamHandler(sys.stdout).push_application()\n",
    "log = Logger('Logbook')\n",
    "\n",
    "import utils; reload(utils)\n",
    "from utils import *\n",
    "import gc\n",
    "# from __future__ import division, print_function\n",
    "from theano.sandbox import cuda\n",
    "from vgg16bn import Vgg16BN\n",
    "from sklearn import metrics\n",
    "\n",
    "def accuracyfunc(y_act, y_pred):\n",
    "    return metrics.accuracy_score(np.argmax(y_act, axis=1), np.argmax(y_pred, axis=1))\n",
    "\n",
    "# Set Parameters and check files\n",
    "input_exists = True\n",
    "log.info('Set Paramters')\n",
    "path = \"../data/fish/\"\n",
    "batch_size=64\n",
    "\n",
    "# Read in our VGG pretrained model\n",
    "log.info('Get VGG')\n",
    "model = vgg_ft_bn(8)\n",
    "\n",
    "# Create our VGG model\n",
    "log.info('Create VGG')\n",
    "vgg640 = Vgg16BN((360, 640)).model\n",
    "vgg640.pop()\n",
    "vgg640.input_shape, vgg640.output_shape\n",
    "vgg640.compile(Adam(), 'categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# get labels\n",
    "(val_classes, trn_classes, val_labels, trn_labels,\n",
    "    val_filenames, filenames, test_filenames) = get_classes(path)\n",
    "\n",
    "# Read in filenames\n",
    "log.info('Read filenames')\n",
    "raw_filenames = [f.split('/')[-1] for f in filenames]\n",
    "raw_test_filenames = [f.split('/')[-1] for f in test_filenames]\n",
    "raw_val_filenames = [f.split('/')[-1] for f in val_filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-02-06 20:45:05.103309] INFO: Logbook: Read in data\n"
     ]
    }
   ],
   "source": [
    "log.info('Read in data')\n",
    "if not input_exists:\n",
    "\n",
    "    batches = get_batches(path+'train', batch_size=batch_size)\n",
    "    val_batches = get_batches(path+'valid', batch_size=batch_size*2, shuffle=False)\n",
    "    (val_classes, trn_classes, val_labels, trn_labels, \n",
    "        val_filenames, filenames, test_filenames) = get_classes(path)\n",
    "    \n",
    "    \n",
    "    # Fetch our large images \n",
    "    log.info('Fetch images')\n",
    "    trn = get_data(path+'train', (360,640))\n",
    "    val = get_data(path+'valid', (360,640))\n",
    "    test = get_data(path+'test', (360,640))\n",
    "    \n",
    "    # Precompute the output of the convolutional part of VGG\n",
    "    log.info('Get VGG output')\n",
    "    conv_val_feat = vgg640.predict(val, batch_size=32, verbose=1)\n",
    "    conv_trn_feat = vgg640.predict(trn, batch_size=32, verbose=1)\n",
    "    conv_test_feat = vgg640.predict(test, batch_size=32, verbose=1)\n",
    "    log.info('Write VGG output')\n",
    "    save_array(path+'results/conv_val_feat.dat', conv_val_feat)\n",
    "    save_array(path+'results/conv_trn_feat.dat', conv_trn_feat) \n",
    "    save_array(path+'results/conv_test_feat.dat', conv_test_feat)     \n",
    "\n",
    "    # For memory purposes delete out the original train and validation\n",
    "    log.info('Clear up memory')\n",
    "    del trn, val, test\n",
    "    gc.collect()\n",
    "\n",
    "conv_val_feat = load_array(path+'results/conv_val_feat.dat')\n",
    "conv_trn_feat = load_array(path+'results/conv_trn_feat.dat') \n",
    "conv_test_feat = load_array(path+'results/conv_test_feat.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-02-06 22:12:27.292619] INFO: Logbook: Create and fit CNN\n"
     ]
    }
   ],
   "source": [
    "# Our Convolutional Net Architecture\n",
    "log.info('Create and fit CNN')\n",
    "def get_lrg_layers():\n",
    "    return [\n",
    "        BatchNormalization(axis=1, input_shape=conv_layers[-1].output_shape[1:]),\n",
    "        Convolution2D(nf,3,3, activation='relu', border_mode='same'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D(),\n",
    "        Convolution2D(nf,3,3, activation='relu', border_mode='same'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D(),\n",
    "        Convolution2D(nf,3,3, activation='relu', border_mode='same'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D((1,2)),\n",
    "        Convolution2D(8,3,3, border_mode='same'),\n",
    "        Dropout(p),\n",
    "        GlobalAveragePooling2D(),\n",
    "        Activation('softmax')\n",
    "    ]\n",
    "\n",
    "# Set up the fully convolutional net (FCN); \n",
    "conv_layers,_ = split_at(vgg640, Convolution2D)\n",
    "nf=128; p=0. # No dropout\n",
    "\n",
    "lrg_model = []\n",
    "predsls = []\n",
    "pvalsls = []\n",
    "bags = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-02-06 22:12:29.630698] INFO: Logbook: Train round0\n",
      "th\n",
      "Train on 3277 samples, validate on 500 samples\n",
      "Epoch 1/2\n",
      "3277/3277 [==============================] - 16s - loss: 0.6650 - acc: 0.7882 - val_loss: 1.1273 - val_acc: 0.6920\n",
      "Epoch 2/2\n",
      "3277/3277 [==============================] - 16s - loss: 0.1135 - acc: 0.9689 - val_loss: 0.3739 - val_acc: 0.9220\n",
      "Train on 3277 samples, validate on 500 samples\n",
      "Epoch 1/6\n",
      "3277/3277 [==============================] - 16s - loss: 0.0348 - acc: 0.9927 - val_loss: 0.2370 - val_acc: 0.9600\n",
      "Epoch 2/6\n",
      "3277/3277 [==============================] - 16s - loss: 0.0090 - acc: 0.9994 - val_loss: 0.1620 - val_acc: 0.9660\n",
      "Epoch 3/6\n",
      "3277/3277 [==============================] - 16s - loss: 0.0196 - acc: 0.9963 - val_loss: 0.2010 - val_acc: 0.9520\n",
      "Epoch 4/6\n",
      "3277/3277 [==============================] - 16s - loss: 0.0068 - acc: 0.9991 - val_loss: 0.2157 - val_acc: 0.9420\n",
      "Epoch 5/6\n",
      "3277/3277 [==============================] - 16s - loss: 0.0041 - acc: 0.9991 - val_loss: 0.1555 - val_acc: 0.9700\n",
      "Epoch 6/6\n",
      "3277/3277 [==============================] - 16s - loss: 0.0312 - acc: 0.9921 - val_loss: 0.4165 - val_acc: 0.9100\n",
      "[2017-02-06 22:14:55.728730] INFO: Logbook: Output Prediction\n",
      "[2017-02-06 22:15:01.140615] INFO: Logbook: Bagged Validation Logloss 0.418\n",
      "[2017-02-06 22:15:01.141530] INFO: Logbook: Bagged Validation Logloss 0.910\n",
      "[2017-02-06 22:15:01.142163] INFO: Logbook: Train round1\n",
      "th\n",
      "Train on 3277 samples, validate on 500 samples\n",
      "Epoch 1/2\n",
      "3277/3277 [==============================] - 16s - loss: 0.6838 - acc: 0.7791 - val_loss: 0.8670 - val_acc: 0.7520\n",
      "Epoch 2/2\n",
      "3277/3277 [==============================] - 16s - loss: 0.1259 - acc: 0.9689 - val_loss: 0.3080 - val_acc: 0.9220\n",
      "Train on 3277 samples, validate on 500 samples\n",
      "Epoch 1/6\n",
      "3277/3277 [==============================] - 16s - loss: 0.0233 - acc: 0.9966 - val_loss: 0.2768 - val_acc: 0.9200\n",
      "Epoch 2/6\n",
      "3277/3277 [==============================] - 16s - loss: 0.0114 - acc: 0.9969 - val_loss: 0.2582 - val_acc: 0.9380\n",
      "Epoch 3/6\n",
      "3277/3277 [==============================] - 16s - loss: 0.0125 - acc: 0.9979 - val_loss: 0.1528 - val_acc: 0.9620\n",
      "Epoch 4/6\n",
      "3277/3277 [==============================] - 16s - loss: 0.0069 - acc: 0.9988 - val_loss: 0.1237 - val_acc: 0.9760\n",
      "Epoch 5/6\n",
      "3277/3277 [==============================] - 16s - loss: 0.0036 - acc: 0.9994 - val_loss: 0.1499 - val_acc: 0.9680\n",
      "Epoch 6/6\n",
      "3277/3277 [==============================] - 16s - loss: 0.1115 - acc: 0.9689 - val_loss: 0.4129 - val_acc: 0.8900\n",
      "[2017-02-06 22:17:26.563997] INFO: Logbook: Output Prediction\n",
      "[2017-02-06 22:17:31.927831] INFO: Logbook: Bagged Validation Logloss 0.300\n",
      "[2017-02-06 22:17:31.928760] INFO: Logbook: Bagged Validation Logloss 0.934\n",
      "[2017-02-06 22:17:31.929374] INFO: Logbook: Train round2\n",
      "th\n",
      "Train on 3277 samples, validate on 500 samples\n",
      "Epoch 1/2\n",
      "3277/3277 [==============================] - 16s - loss: 0.7188 - acc: 0.7681 - val_loss: 1.3256 - val_acc: 0.6600\n",
      "Epoch 2/2\n",
      "3277/3277 [==============================] - 16s - loss: 0.1324 - acc: 0.9609 - val_loss: 0.8631 - val_acc: 0.6560\n",
      "Train on 3277 samples, validate on 500 samples\n",
      "Epoch 1/6\n",
      "3277/3277 [==============================] - 16s - loss: 0.0472 - acc: 0.9878 - val_loss: 0.1920 - val_acc: 0.9640\n",
      "Epoch 2/6\n",
      "3277/3277 [==============================] - 16s - loss: 0.0176 - acc: 0.9963 - val_loss: 0.1841 - val_acc: 0.9540\n",
      "Epoch 3/6\n",
      "3277/3277 [==============================] - 16s - loss: 0.0077 - acc: 0.9988 - val_loss: 0.1414 - val_acc: 0.9680\n",
      "Epoch 4/6\n",
      "3277/3277 [==============================] - 16s - loss: 0.0027 - acc: 0.9997 - val_loss: 0.1534 - val_acc: 0.9680\n",
      "Epoch 5/6\n",
      "3277/3277 [==============================] - 16s - loss: 0.0012 - acc: 1.0000 - val_loss: 0.1422 - val_acc: 0.9720\n",
      "Epoch 6/6\n",
      "3277/3277 [==============================] - 16s - loss: 5.5548e-04 - acc: 1.0000 - val_loss: 0.1385 - val_acc: 0.9760\n",
      "[2017-02-06 22:19:56.682154] INFO: Logbook: Output Prediction\n",
      "[2017-02-06 22:20:03.208133] INFO: Logbook: Bagged Validation Logloss 0.179\n",
      "[2017-02-06 22:20:03.209113] INFO: Logbook: Bagged Validation Logloss 0.956\n",
      "[2017-02-06 22:20:03.209863] INFO: Logbook: Train round3\n",
      "th\n",
      "Train on 3277 samples, validate on 500 samples\n",
      "Epoch 1/2\n",
      "3277/3277 [==============================] - 16s - loss: 0.6318 - acc: 0.7946 - val_loss: 1.0105 - val_acc: 0.6780\n",
      "Epoch 2/2\n",
      "3277/3277 [==============================] - 16s - loss: 0.1195 - acc: 0.9667 - val_loss: 0.5157 - val_acc: 0.8300\n",
      "Train on 3277 samples, validate on 500 samples\n",
      "Epoch 1/6\n",
      "3277/3277 [==============================] - 16s - loss: 0.0496 - acc: 0.9878 - val_loss: 0.3487 - val_acc: 0.9080\n",
      "Epoch 2/6\n",
      "3277/3277 [==============================] - 16s - loss: 0.0286 - acc: 0.9927 - val_loss: 0.1871 - val_acc: 0.9600\n",
      "Epoch 3/6\n",
      "3277/3277 [==============================] - 16s - loss: 0.0109 - acc: 0.9982 - val_loss: 0.2013 - val_acc: 0.9580\n",
      "Epoch 4/6\n",
      "3277/3277 [==============================] - 16s - loss: 0.0173 - acc: 0.9988 - val_loss: 0.1555 - val_acc: 0.9740\n",
      "Epoch 5/6\n",
      "3277/3277 [==============================] - 16s - loss: 0.0156 - acc: 0.9991 - val_loss: 0.1548 - val_acc: 0.9760\n",
      "Epoch 6/6\n",
      "3277/3277 [==============================] - 16s - loss: 0.0155 - acc: 0.9991 - val_loss: 0.1611 - val_acc: 0.9720\n",
      "[2017-02-06 22:22:28.163984] INFO: Logbook: Output Prediction\n",
      "[2017-02-06 22:22:33.419303] INFO: Logbook: Bagged Validation Logloss 0.157\n",
      "[2017-02-06 22:22:33.420201] INFO: Logbook: Bagged Validation Logloss 0.970\n",
      "[2017-02-06 22:22:33.420978] INFO: Logbook: Train round4\n",
      "th\n",
      "Train on 3277 samples, validate on 500 samples\n",
      "Epoch 1/2\n",
      "3277/3277 [==============================] - 16s - loss: 0.6395 - acc: 0.7897 - val_loss: 1.1919 - val_acc: 0.6420\n",
      "Epoch 2/2\n",
      "3277/3277 [==============================] - 16s - loss: 0.1078 - acc: 0.9713 - val_loss: 0.4303 - val_acc: 0.8520\n",
      "Train on 3277 samples, validate on 500 samples\n",
      "Epoch 1/6\n",
      "3277/3277 [==============================] - 16s - loss: 0.0329 - acc: 0.9939 - val_loss: 0.2546 - val_acc: 0.9280\n",
      "Epoch 2/6\n",
      "3277/3277 [==============================] - 16s - loss: 0.0100 - acc: 0.9982 - val_loss: 0.1962 - val_acc: 0.9580\n",
      "Epoch 3/6\n",
      "3277/3277 [==============================] - 16s - loss: 0.0061 - acc: 0.9994 - val_loss: 0.1831 - val_acc: 0.9680\n",
      "Epoch 4/6\n",
      "3277/3277 [==============================] - 16s - loss: 0.0193 - acc: 0.9969 - val_loss: 0.2378 - val_acc: 0.9500\n",
      "Epoch 5/6\n",
      "3277/3277 [==============================] - 16s - loss: 0.0162 - acc: 0.9966 - val_loss: 0.2658 - val_acc: 0.9480\n",
      "Epoch 6/6\n",
      "3277/3277 [==============================] - 16s - loss: 0.0190 - acc: 0.9951 - val_loss: 0.3537 - val_acc: 0.9180\n",
      "[2017-02-06 22:24:59.451185] INFO: Logbook: Output Prediction\n",
      "[2017-02-06 22:25:06.742786] INFO: Logbook: Bagged Validation Logloss 0.163\n",
      "[2017-02-06 22:25:06.743802] INFO: Logbook: Bagged Validation Logloss 0.972\n",
      "[2017-02-06 22:25:06.744580] INFO: Logbook: Train round5\n",
      "th\n",
      "Train on 3277 samples, validate on 500 samples\n",
      "Epoch 1/2\n",
      "3277/3277 [==============================] - 16s - loss: 0.6949 - acc: 0.7769 - val_loss: 0.7468 - val_acc: 0.7840\n",
      "Epoch 2/2\n",
      "3277/3277 [==============================] - 16s - loss: 0.1280 - acc: 0.9628 - val_loss: 0.3317 - val_acc: 0.9340\n",
      "Train on 3277 samples, validate on 500 samples\n",
      "Epoch 1/6\n",
      "3277/3277 [==============================] - 16s - loss: 0.0377 - acc: 0.9921 - val_loss: 0.2556 - val_acc: 0.9320\n",
      "Epoch 2/6\n",
      "3277/3277 [==============================] - 16s - loss: 0.0211 - acc: 0.9976 - val_loss: 0.1576 - val_acc: 0.9560\n",
      "Epoch 3/6\n",
      "3277/3277 [==============================] - 16s - loss: 0.0044 - acc: 0.9994 - val_loss: 0.1346 - val_acc: 0.9700\n",
      "Epoch 4/6\n",
      "3277/3277 [==============================] - 16s - loss: 0.0019 - acc: 1.0000 - val_loss: 0.1411 - val_acc: 0.9740\n",
      "Epoch 5/6\n",
      "3277/3277 [==============================] - 16s - loss: 0.0020 - acc: 0.9988 - val_loss: 0.1594 - val_acc: 0.9700\n",
      "Epoch 6/6\n",
      "3277/3277 [==============================] - 16s - loss: 0.0073 - acc: 0.9991 - val_loss: 0.2216 - val_acc: 0.9600\n",
      "[2017-02-06 22:27:31.495932] INFO: Logbook: Output Prediction\n",
      "[2017-02-06 22:27:39.066756] INFO: Logbook: Bagged Validation Logloss 0.158\n",
      "[2017-02-06 22:27:39.067687] INFO: Logbook: Bagged Validation Logloss 0.976\n",
      "[2017-02-06 22:27:39.068417] INFO: Logbook: Train round6\n",
      "th\n",
      "Train on 3277 samples, validate on 500 samples\n",
      "Epoch 1/2\n",
      "3277/3277 [==============================] - 16s - loss: 0.6742 - acc: 0.7867 - val_loss: 1.1251 - val_acc: 0.7480\n",
      "Epoch 2/2\n",
      "3277/3277 [==============================] - 16s - loss: 0.1305 - acc: 0.9661 - val_loss: 0.4405 - val_acc: 0.8760\n",
      "Train on 3277 samples, validate on 500 samples\n",
      "Epoch 1/6\n",
      "3277/3277 [==============================] - 16s - loss: 0.0389 - acc: 0.9918 - val_loss: 0.1967 - val_acc: 0.9520\n",
      "Epoch 2/6\n",
      "3277/3277 [==============================] - 16s - loss: 0.0151 - acc: 0.9969 - val_loss: 0.1494 - val_acc: 0.9680\n",
      "Epoch 3/6\n",
      "3277/3277 [==============================] - 16s - loss: 0.0086 - acc: 0.9988 - val_loss: 0.1556 - val_acc: 0.9560\n",
      "Epoch 4/6\n",
      "3277/3277 [==============================] - 16s - loss: 0.0156 - acc: 0.9966 - val_loss: 0.1939 - val_acc: 0.9500\n",
      "Epoch 5/6\n",
      "3277/3277 [==============================] - 16s - loss: 0.0088 - acc: 0.9988 - val_loss: 0.1347 - val_acc: 0.9640\n",
      "Epoch 6/6\n",
      "3277/3277 [==============================] - 16s - loss: 0.0042 - acc: 0.9988 - val_loss: 0.1710 - val_acc: 0.9700\n",
      "[2017-02-06 22:30:03.891361] INFO: Logbook: Output Prediction\n",
      "[2017-02-06 22:30:12.083001] INFO: Logbook: Bagged Validation Logloss 0.151\n",
      "[2017-02-06 22:30:12.083929] INFO: Logbook: Bagged Validation Logloss 0.974\n",
      "[2017-02-06 22:30:12.084677] INFO: Logbook: Train round7\n",
      "th\n",
      "Train on 3277 samples, validate on 500 samples\n",
      "Epoch 1/2\n",
      "2624/3277 [=======================>......] - ETA: 3s - loss: 0.7334 - acc: 0.7736"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-3ca854bd6fa9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlrg_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     lrg_model[i].fit(conv_trn_feat, trn_labels, batch_size=batch_size, nb_epoch=2, \n\u001b[0;32m----> 7\u001b[0;31m                  validation_data=(conv_val_feat, val_labels))\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mlrg_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     lrg_model[i].fit(conv_trn_feat, trn_labels, batch_size=batch_size, nb_epoch=6,\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m                               \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m                               sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight)\u001b[0m\n\u001b[1;32m   1104\u001b[0m                               \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m                               callback_metrics=callback_metrics)\n\u001b[0m\u001b[1;32m   1107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics)\u001b[0m\n\u001b[1;32m    822\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    825\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 717\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(bags):\n",
    "    log.info('Train round' + str(i))\n",
    "    lrg_model.append(Sequential(get_lrg_layers()))\n",
    "    # lrg_model.summary()\n",
    "    lrg_model[i].compile(Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    lrg_model[i].fit(conv_trn_feat, trn_labels, batch_size=batch_size, nb_epoch=2, \n",
    "                 validation_data=(conv_val_feat, val_labels))\n",
    "    lrg_model[i].optimizer.lr=1e-5\n",
    "    lrg_model[i].fit(conv_trn_feat, trn_labels, batch_size=batch_size, nb_epoch=6,\n",
    "                 validation_data=(conv_val_feat, val_labels))\n",
    "\n",
    "    ## Evaluate the model\n",
    "    #log.info('Evaluate')\n",
    "    #lrg_model[i].evaluate(conv_val_feat, val_labels)\n",
    "\n",
    "    # Make our prediction on the lrg_model layer\n",
    "    log.info('Output Prediction')\n",
    "    predsls.append(lrg_model[i].predict(conv_test_feat, batch_size=batch_size)) # or try 32 batch_size\n",
    "    pvalsls.append(lrg_model[i].predict(conv_val_feat, batch_size=batch_size))\n",
    "    val_score = \"%.3f\" % metrics.log_loss(val_labels, sum(pvalsls)/len(pvalsls))\n",
    "    acc_score = \"%.3f\" % accuracyfunc(val_labels, do_clip(sum(pvalsls)/len(pvalsls), .99))\n",
    "    log.info('Bagged Validation Logloss ' + str(val_score))\n",
    "    log.info('Bagged Validation Logloss ' + str(acc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18894760163128377"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.log_loss(val_labels, do_clip(sum(pvalsls)/len(pvalsls), .99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = sum(predsls)/len(predsls)\n",
    "subm = do_clip(preds,.99)\n",
    "subm_name = path+'results/subm_bb_conv_lrg0206C.csv.gz'\n",
    "pred_name = path+'results/pred_bb_conv_lrg0206C.csv.gz'\n",
    "\n",
    "classes = ['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT']\n",
    "submission = pd.DataFrame(subm, columns=classes)\n",
    "submission.insert(0, 'image', raw_test_filenames)\n",
    "submission.to_csv(subm_name, index=False, compression='gzip')\n",
    "subm1 = pd.DataFrame(preds, columns=classes)\n",
    "subm1.insert(0, 'image', raw_test_filenames)\n",
    "subm1.to_csv(pred_name, index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='../data/fish/results/subm_bb_conv_lrg0206C.csv.gz' target='_blank'>../data/fish/results/subm_bb_conv_lrg0206C.csv.gz</a><br>"
      ],
      "text/plain": [
       "/home/ubuntu/fish/data/fish/results/subm_bb_conv_lrg0206C.csv.gz"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FileLink(subm_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.preprocessing.image.ImageDataGenerator at 0x7fc7362a03d0>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = image.ImageDataGenerator(horizontal_flip=True)\n",
    "gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3277 images belonging to 8 classes.\n",
      "Found 3277 images belonging to 8 classes.\n",
      "Found 500 images belonging to 8 classes.\n",
      "Found 3277 images belonging to 8 classes.\n",
      "Found 500 images belonging to 8 classes.\n",
      "Found 1000 images belonging to 1 classes.\n",
      "[2017-02-06 22:43:14.473802] INFO: Logbook: Fetch images\n",
      "Found 3277 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "input_exists =False\n",
    "if not input_exists:\n",
    "\n",
    "    batches = get_batches(path+'train',gen, batch_size=batch_size)\n",
    "    batchestmp = get_batches(path+'train', batch_size=batch_size)\n",
    "    val_batches = get_batches(path+'valid', batch_size=batch_size*2, shuffle=False)\n",
    "    (val_classes, trn_classes, val_labels, trn_labels, \n",
    "        val_filenames, filenames, test_filenames) = get_classes(path)\n",
    "    \n",
    "    \n",
    "    # Fetch our large images \n",
    "    log.info('Fetch images')\n",
    "    trn = get_data(path+'train', (360,640))\n",
    "    val = get_data(path+'valid', (360,640))\n",
    "    test = get_data(path+'test', (360,640))\n",
    "    \n",
    "    # Precompute the output of the convolutional part of VGG\n",
    "    log.info('Get VGG output')\n",
    "    conv_val_feat = vgg640.predict(val, batch_size=32, verbose=1)\n",
    "    conv_trn_feat = vgg640.predict(trn, batch_size=32, verbose=1)\n",
    "    conv_test_feat = vgg640.predict(test, batch_size=32, verbose=1)\n",
    "    log.info('Write VGG output')\n",
    "    save_array(path+'results/conv_val_feat.dat', conv_val_feat)\n",
    "    save_array(path+'results/conv_trn_feat.dat', conv_trn_feat) \n",
    "    save_array(path+'results/conv_test_feat.dat', conv_test_feat)     \n",
    "\n",
    "    # For memory purposes delete out the original train and validation\n",
    "    log.info('Clear up memory')\n",
    "    del trn, val, test\n",
    "    gc.collect()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
