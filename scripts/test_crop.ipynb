{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "# Read in Libraries\n",
    "from __future__ import division, print_function\n",
    "from logbook import Logger, StreamHandler\n",
    "import sys\n",
    "StreamHandler(sys.stdout).push_application()\n",
    "log = Logger('Logbook')\n",
    "import shutil, csv, time\n",
    "timestr = time.strftime(\"%Y%m%d\")\n",
    "import ujson as json\n",
    "import utils; reload(utils)\n",
    "from utils import *\n",
    "import gc\n",
    "# from __future__ import division, print_function\n",
    "from theano.sandbox import cuda\n",
    "from vgg16bn import Vgg16BN\n",
    "from sklearn import metrics\n",
    "%matplotlib inline\n",
    "\n",
    "def accuracyfunc(y_act, y_pred):\n",
    "    return metrics.accuracy_score(np.argmax(y_act, axis=1), np.argmax(y_pred, axis=1))\n",
    "    \n",
    "def refresh_directory_structure(name, sub_dirs):\n",
    "    gdir = os.path.join(path, name)\n",
    "    if os.path.exists(gdir):\n",
    "        shutil.rmtree(gdir)\n",
    "    os.makedirs(gdir)\n",
    "    for sub_dir in sub_dirs:\n",
    "        os.makedirs(os.path.join(gdir, sub_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-03-03 21:57:57.441045] INFO: Logbook: Set Paramters\n"
     ]
    }
   ],
   "source": [
    "# Set Parameters and check files\n",
    "refresh_directories = False\n",
    "input_exists = False\n",
    "full = False\n",
    "log.info('Set Paramters')\n",
    "path = \"../data/fish/crop/\"\n",
    "batch_size=32\n",
    "clip = 0.99\n",
    "bags = 1\n",
    "load_size = (300,300)#(360, 640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-03-03 21:57:57.735750] INFO: Logbook: Get VGG\n",
      "[2017-03-03 21:58:01.006430] INFO: Logbook: Create VGG\n",
      "Found 2685 images belonging to 8 classes.\n",
      "Found 622 images belonging to 8 classes.\n",
      "Found 721 images belonging to 1 classes.\n",
      "[2017-03-03 21:58:01.839349] INFO: Logbook: Read filenames\n"
     ]
    }
   ],
   "source": [
    "# Read in our VGG pretrained model\n",
    "log.info('Get VGG')\n",
    "model = vgg_ft_bn(8)\n",
    "\n",
    "# Create our VGG model\n",
    "log.info('Create VGG')\n",
    "vgg640 = Vgg16BN(load_size).model\n",
    "vgg640.pop()\n",
    "vgg640.input_shape, vgg640.output_shape\n",
    "vgg640.compile(Adam(), 'categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# get labels\n",
    "(val_classes, trn_classes, val_labels, trn_labels,\n",
    "    val_filenames, filenames, test_filenames) = get_classes(path)\n",
    "\n",
    "# Read in filenames\n",
    "log.info('Read filenames')\n",
    "raw_filenames = [f.split('/')[-1] for f in filenames]\n",
    "raw_test_filenames = [f.split('/')[-1] for f in test_filenames]\n",
    "raw_val_filenames = [f.split('/')[-1] for f in val_filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-03-03 21:58:01.872001] INFO: Logbook: Read in data\n",
      "Found 2685 images belonging to 8 classes.\n",
      "Found 622 images belonging to 8 classes.\n",
      "Found 2685 images belonging to 8 classes.\n",
      "Found 622 images belonging to 8 classes.\n",
      "Found 721 images belonging to 1 classes.\n",
      "[2017-03-03 21:58:01.907724] INFO: Logbook: Fetch images\n",
      "[2017-03-03 21:58:01.908376] INFO: Logbook: Get VGG output\n",
      "[2017-03-03 21:58:01.909034] INFO: Logbook: Write VGG output\n",
      "Found 721 images belonging to 1 classes.\n",
      "721/721 [==============================] - 33s    \n",
      "Found 622 images belonging to 8 classes.\n",
      "622/622 [==============================] - 29s    \n",
      "Found 2685 images belonging to 8 classes.\n",
      "2685/2685 [==============================] - 126s   \n",
      "[2017-03-03 22:04:49.711088] INFO: Logbook: Clear up memory\n"
     ]
    }
   ],
   "source": [
    "log.info('Read in data')\n",
    "if not input_exists:\n",
    "\n",
    "    batches = get_batches(path+'train', batch_size=batch_size)\n",
    "    val_batches = get_batches(path+'valid', batch_size=batch_size*2, shuffle=False)\n",
    "    (val_classes, trn_classes, val_labels, trn_labels, \n",
    "        val_filenames, filenames, test_filenames) = get_classes(path)\n",
    "    \n",
    "    # Fetch our large images \n",
    "    # Precompute the output of the convolutional part of VGG\n",
    "    log.info('Fetch images')\n",
    "    log.info('Get VGG output')\n",
    "    log.info('Write VGG output')\n",
    "    \n",
    "    test = get_data(path+'test', load_size)\n",
    "    conv_test_feat = vgg640.predict(test, batch_size=16, verbose=1)\n",
    "    save_array(path+'../results/conv_test_crop_feat.dat', conv_test_feat)     \n",
    "    del test, conv_test_feat\n",
    "    gc.collect()\n",
    "    \n",
    "    val = get_data(path+'valid', load_size)\n",
    "    conv_val_feat = vgg640.predict(val, batch_size=16, verbose=1)\n",
    "    save_array(path+'../results/conv_val_crop_feat.dat', conv_val_feat)\n",
    "    del val, conv_val_feat\n",
    "    gc.collect()\n",
    "    \n",
    "    trn = get_data(path+'train', load_size)\n",
    "    conv_trn_feat = vgg640.predict(trn, batch_size=16, verbose=1)    \n",
    "    del trn\n",
    "    gc.collect()\n",
    "    save_array(path+'../results/conv_trn_crop_feat.dat', conv_trn_feat) \n",
    "    del conv_trn_feat\n",
    "    gc.collect()\n",
    "\n",
    "    # For memory purposes delete out the original train and validation\n",
    "    log.info('Clear up memory')\n",
    "    #del trn, val, test\n",
    "    gc.collect()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "conv_val_feat = load_array(path+'../results/conv_val_crop_feat.dat')\n",
    "conv_trn_feat = load_array(path+'../results/conv_trn_crop_feat.dat') \n",
    "conv_test_feat = load_array(path+'../results/conv_test_crop_feat.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classes = ['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT']\n",
    "def fish_only(mat):\n",
    "    return np.delete(mat, 4, axis=1)\n",
    "\n",
    "trn_of_labels = fish_only(trn_labels)\n",
    "val_of_labels = fish_only(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2685, 7)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_of_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-03-03 22:04:52.263036] INFO: Logbook: Create and fit CNN\n"
     ]
    }
   ],
   "source": [
    "if full:\n",
    "    conv_trn_feat = np.concatenate([conv_trn_feat, conv_val_feat])\n",
    "    trn_labels = np.concatenate([trn_of_labels, val_of_labels]) \n",
    "    #trn_bbox = np.concatenate([trn_bbox, val_bbox])\n",
    "    \n",
    "# Our Convolutional Net Architecture\n",
    "log.info('Create and fit CNN')\n",
    "p=0.6\n",
    "# Set up the fully convolutional net (FCN); \n",
    "conv_layers,_ = split_at(vgg640, Convolution2D)\n",
    "nf=128; p=0. # No dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 18, 18)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_layers[-1].output_shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th\n",
      "Train on 2685 samples, validate on 622 samples\n",
      "Epoch 1/2\n",
      "2685/2685 [==============================] - 9s - loss: 1.5867 - acc: 0.5088 - val_loss: 1.8362 - val_acc: 0.3360\n",
      "Epoch 2/2\n",
      "2685/2685 [==============================] - 9s - loss: 1.2002 - acc: 0.6436 - val_loss: 1.5003 - val_acc: 0.5241\n",
      "Train on 2685 samples, validate on 622 samples\n",
      "Epoch 1/10\n",
      "2685/2685 [==============================] - 9s - loss: 1.0135 - acc: 0.6901 - val_loss: 1.3899 - val_acc: 0.5257\n",
      "Epoch 2/10\n",
      "2685/2685 [==============================] - 9s - loss: 0.8761 - acc: 0.7292 - val_loss: 1.3343 - val_acc: 0.5338\n",
      "Epoch 3/10\n",
      "2685/2685 [==============================] - 9s - loss: 0.7553 - acc: 0.7743 - val_loss: 1.2554 - val_acc: 0.5354\n",
      "Epoch 4/10\n",
      "2685/2685 [==============================] - 9s - loss: 0.6565 - acc: 0.8220 - val_loss: 1.2086 - val_acc: 0.5611\n",
      "Epoch 5/10\n",
      "2685/2685 [==============================] - 9s - loss: 0.5671 - acc: 0.8574 - val_loss: 1.1821 - val_acc: 0.5788\n",
      "Epoch 6/10\n",
      "2685/2685 [==============================] - 9s - loss: 0.4837 - acc: 0.8849 - val_loss: 1.1387 - val_acc: 0.5949\n",
      "Epoch 7/10\n",
      "2685/2685 [==============================] - 9s - loss: 0.4098 - acc: 0.9132 - val_loss: 1.0919 - val_acc: 0.5981\n",
      "Epoch 8/10\n",
      "2685/2685 [==============================] - 9s - loss: 0.3677 - acc: 0.9218 - val_loss: 1.0935 - val_acc: 0.6061\n",
      "Epoch 9/10\n",
      "2685/2685 [==============================] - 9s - loss: 0.3196 - acc: 0.9348 - val_loss: 1.0571 - val_acc: 0.6238\n",
      "Epoch 10/10\n",
      "2685/2685 [==============================] - 9s - loss: 0.2669 - acc: 0.9512 - val_loss: 1.1300 - val_acc: 0.6077\n",
      "Train on 2685 samples, validate on 622 samples\n",
      "Epoch 1/1\n",
      "2685/2685 [==============================] - 9s - loss: 0.2258 - acc: 0.9616 - val_loss: 1.0922 - val_acc: 0.6318\n",
      "[2017-03-03 23:43:21.587073] INFO: Logbook: Bagged Validation Logloss 1.092\n",
      "[2017-03-03 23:43:21.588062] INFO: Logbook: Bagged Validation Accuracy 0.632\n",
      "th\n",
      "Train on 2685 samples, validate on 622 samples\n",
      "Epoch 1/2\n",
      "2685/2685 [==============================] - 9s - loss: 1.5711 - acc: 0.5028 - val_loss: 1.7807 - val_acc: 0.3537\n",
      "Epoch 2/2\n",
      "2685/2685 [==============================] - 9s - loss: 1.1547 - acc: 0.6507 - val_loss: 1.3975 - val_acc: 0.4952\n",
      "Train on 2685 samples, validate on 622 samples\n",
      "Epoch 1/10\n",
      "2685/2685 [==============================] - 9s - loss: 0.9733 - acc: 0.6853 - val_loss: 1.3030 - val_acc: 0.5257\n",
      "Epoch 2/10\n",
      "2685/2685 [==============================] - 9s - loss: 0.8423 - acc: 0.7322 - val_loss: 1.2356 - val_acc: 0.5627\n",
      "Epoch 3/10\n",
      "2685/2685 [==============================] - 9s - loss: 0.7299 - acc: 0.7769 - val_loss: 1.1933 - val_acc: 0.5740\n",
      "Epoch 4/10\n",
      "2685/2685 [==============================] - 9s - loss: 0.6314 - acc: 0.8227 - val_loss: 1.1460 - val_acc: 0.5836\n",
      "Epoch 5/10\n",
      "2685/2685 [==============================] - 9s - loss: 0.5365 - acc: 0.8585 - val_loss: 1.1070 - val_acc: 0.5884\n",
      "Epoch 6/10\n",
      "2685/2685 [==============================] - 9s - loss: 0.4732 - acc: 0.8767 - val_loss: 1.0802 - val_acc: 0.6013\n",
      "Epoch 7/10\n",
      "2685/2685 [==============================] - 9s - loss: 0.4041 - acc: 0.9050 - val_loss: 1.0667 - val_acc: 0.6302\n",
      "Epoch 8/10\n",
      "2685/2685 [==============================] - 9s - loss: 0.3517 - acc: 0.9236 - val_loss: 1.0732 - val_acc: 0.6302\n",
      "Epoch 9/10\n",
      "2685/2685 [==============================] - 9s - loss: 0.2982 - acc: 0.9348 - val_loss: 1.0172 - val_acc: 0.6415\n",
      "Epoch 10/10\n",
      "1408/2685 [==============>...............] - ETA: 4s - loss: 0.2603 - acc: 0.9474"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "nf = 1024\n",
    "p  = 0.6\n",
    "def create_model():\n",
    "    inp = Input(conv_layers[-1].output_shape[1:])\n",
    "    x = MaxPooling2D()(inp)\n",
    "    #x = ZeroPadding2D((1,1))(x)\n",
    "    #x = Convolution2D(64,3,3, activation='relu', border_mode='same')(x)\n",
    "    #x = Convolution2D(64,3,3, activation='relu', border_mode='same')(x)\n",
    "    #x =   Dropout(p/4)(x)\n",
    "    #x = BatchNormalization(axis=1)(x)\n",
    "    x = MaxPooling2D()(x)\n",
    "    x = ZeroPadding2D((1,1))(x)\n",
    "    x = Convolution2D(512,3,3, activation='relu', border_mode='same')(x)\n",
    "    #x = ZeroPadding2D((1,1))(x)\n",
    "    #x = BatchNormalization(axis=1)(x)\n",
    "    #x = Convolution2D(512,3,3, activation='relu', border_mode='same')(x)\n",
    "    x =   Dropout(p/4)(x)\n",
    "    x = BatchNormalization(axis=1)(x)\n",
    "    x = MaxPooling2D()(x)\n",
    "    x = ZeroPadding2D((1,1))(x)\n",
    "    x = Convolution2D(1024,3,3, activation='relu', border_mode='same')(x)\n",
    "    #x = ZeroPadding2D((1,1))(x)\n",
    "    #x = BatchNormalization(axis=1)(x)\n",
    "    #x = Convolution2D(1024,3,3, activation='relu', border_mode='same')(x)\n",
    "    x =   Dropout(p)(x)\n",
    "    x = BatchNormalization(axis=1)(x)\n",
    "    #x1 =   MaxPooling2D()(x)\n",
    "    x1 =   Convolution2D(7,3,3, border_mode='same')(x)\n",
    "    x1 =   GlobalAveragePooling2D()(x1)\n",
    "    #x = BatchNormalization(axis=1)(x)\n",
    "    #x = Dropout(p)(x)\n",
    "    #x = Flatten()(x)\n",
    "    #x = Dense(1024, activation='relu')(x)\n",
    "    #x = BatchNormalization()(x)\n",
    "    #x = Dropout(p)(x)\n",
    "    #x = Dense(512, activation='relu')(x)\n",
    "    #x = BatchNormalization()(x)\n",
    "    #x = Dropout(p/2)(x)\n",
    "    x_class = Dense(7, activation='softmax', name='class')(x1)\n",
    "    return inp, x_class\n",
    "\n",
    "\n",
    "model = []\n",
    "predsls = []\n",
    "pvalsls = []\n",
    "\n",
    "for ii in range(10):\n",
    "    inp, x_class = create_model()\n",
    "    model.append(Model([inp], [x_class]))\n",
    "    model[ii].compile(Adam(lr=1e-5, decay=1e-6), loss=['categorical_crossentropy'], metrics=['accuracy'])\n",
    "    #model[ii].summary()\n",
    "    model[ii].fit(conv_trn_feat, [trn_of_labels], batch_size=batch_size, nb_epoch=2, \n",
    "                 validation_data=(conv_val_feat, [val_of_labels]))\n",
    "    \n",
    "    model[ii].optimizer.lr = 1e-5\n",
    "    model[ii].fit(conv_trn_feat, [trn_of_labels], batch_size=batch_size, nb_epoch=10, \n",
    "                 validation_data=(conv_val_feat, [val_of_labels]))\n",
    "\n",
    "    count = 0\n",
    "    while count < 1:\n",
    "        model[ii].fit(conv_trn_feat, [trn_of_labels], batch_size=batch_size, nb_epoch=1, \n",
    "                     validation_data=(conv_val_feat, [val_of_labels]))\n",
    "        predsls.append(model[ii].predict(conv_test_feat, batch_size=batch_size)) # or try 32 batch_size\n",
    "        pvalsls.append(model[ii].predict(conv_val_feat, batch_size=batch_size))\n",
    "        val_score = \"%.3f\" % metrics.log_loss(val_of_labels, sum(pvalsls)/len(pvalsls))\n",
    "        acc_score = \"%.3f\" % accuracyfunc(val_of_labels, do_clip(sum(pvalsls)/len(pvalsls), clip))\n",
    "        log.info('Bagged Validation Logloss ' + str(val_score))\n",
    "        log.info('Bagged Validation Accuracy ' + str(acc_score))\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a3cd23421dca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# metrics.log_loss(val_labels, do_clip(sum(pvalsls)/len(pvalsls), .9999))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredsls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredsls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msubm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo_clip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "# metrics.log_loss(val_labels, do_clip(sum(pvalsls)/len(pvalsls), .9999))\n",
    "preds = sum(predsls)/len(predsls)\n",
    "subm = do_clip(preds, clip)\n",
    "\n",
    "if full:\n",
    "    subm_name = path+'../results/subm_full_crop_of_' + timestr + '.csv' #'.csv.gz'\n",
    "else:\n",
    "    subm_name = path+'../results/subm_part_crop_of_' + timestr + '.csv' #'.csv.gz'\n",
    "\n",
    "classes = ['ALB', 'BET', 'DOL', 'LAG', 'OTHER', 'SHARK', 'YFT']\n",
    "submission = pd.DataFrame(subm, columns=classes)\n",
    "submission.insert(0, 'image', raw_test_filenames)\n",
    "submission.to_csv(subm_name, index=False)#, compression='gzip')\n",
    "log.info('Done - files @ ' + subm_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FileLink(subm_name)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
