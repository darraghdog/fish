{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, random, glob, pickle, collections, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ujson as json\n",
    "from PIL import Image\n",
    "import gc\n",
    "import glob\n",
    "import shutil, csv, time\n",
    "\n",
    "import utils; reload(utils)\n",
    "from utils import *\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "from keras.models import Sequential, Model, load_model, model_from_json\n",
    "from keras.layers import GlobalAveragePooling2D, Flatten, Dropout, Dense, LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import image\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_DIR = '../data/fish/train-all/'\n",
    "TEST_DIR =  '../data/fish/test/' #'../RFCN/JPEGImages/'\n",
    "# RFCN_MODEL = 'resnet101_rfcn_ohem_iter_30000'\n",
    "CHECKPOINT_DIR = './checkpoints/checkpoint08/'\n",
    "LOG_DIR = './logs/log08/'\n",
    "FISH_CLASSES = ['NoF', 'ALB', 'BET', 'DOL', 'LAG', 'OTHER', 'SHARK', 'YFT']\n",
    "CONF_THRESH = 0.8\n",
    "ROWS = 224\n",
    "COLS = 224\n",
    "BATCHSIZE = 32 # 256 #64\n",
    "LEARNINGRATE = 1e-4\n",
    "BG_THRESH_HI = 0.3\n",
    "BG_THRESH_LO = 0.1\n",
    "bags = 5\n",
    "learn_round = 2\n",
    "p=16\n",
    "full = True\n",
    "\n",
    "#def load_img(path, bbox, target_size=None):\n",
    "#    img = Image.open(path)\n",
    "#    img = img.convert('RGB')\n",
    "#    cropped = img.crop((bbox[0],bbox[1],bbox[2],bbox[3]))\n",
    "#    if target_size:\n",
    "#        cropped = cropped.resize((target_size[1], target_size[0]))\n",
    "#    return cropped\n",
    "\n",
    "#def load_img(path, bbox, target_size=None):\n",
    "#    img = Image.open(path)\n",
    "#    imsize = Image.open(path).size\n",
    "#    height, width = bbox[2]-bbox[0], bbox[3]-bbox[1]\n",
    "#    length = max(height, width)\n",
    "#    for i in [0,1]:\n",
    "#        if bbox[0+i]+length > imsize[0+i]:\n",
    "#            bbox[0+i] = bbox[2+i] - length\n",
    "#        else:\n",
    "#            bbox[2+i] = bbox[0+i] + length\n",
    "#    img = img.convert('RGB')\n",
    "#    cropped = img.crop((bbox[0],bbox[1],bbox[2],bbox[3]))\n",
    "#    if target_size:\n",
    "#        cropped = cropped.resize((target_size[1], target_size[0]))\n",
    "#    return cropped\n",
    "\n",
    "def load_img(path, bbox, target_size=None):\n",
    "    img = Image.open(path)\n",
    "    imsize = Image.open(path).size\n",
    "    height, width = bbox[2]-bbox[0], bbox[3]-bbox[1]\n",
    "    length = max(height, width)    \n",
    "    # Make it square\n",
    "    dim = [width, height]\n",
    "    for i in range(2):\n",
    "        offset = length - dim[0+i]\n",
    "        if bbox[0+i]+length+(offset/2) > imsize[0+i]:\n",
    "            bbox[0+i] = bbox[2+i] - length + (offset/2)\n",
    "            bbox[2+i] = bbox[2+i] + (offset/2)\n",
    "        else:\n",
    "            bbox[2+i] = bbox[0+i] + length\n",
    "        bbox[0+i] -= length*0.02\n",
    "        bbox[2+i] += length*0.02\n",
    "        \n",
    "    img = img.convert('RGB')\n",
    "    cropped = img.crop((bbox[0],bbox[1],bbox[2],bbox[3]))\n",
    "    if target_size:\n",
    "        cropped = cropped.resize((target_size[1], target_size[0]))\n",
    "    #if height < width:\n",
    "    #    cropped = cropped.rotate(-90)\n",
    "    return cropped\n",
    "\n",
    "def preprocess_input(x):\n",
    "    #resnet50 image preprocessing\n",
    "    # 'RGB'->'BGR'\n",
    "    x = x[:, :, ::-1]\n",
    "    x[:, :, 0] -= 103.939\n",
    "    x[:, :, 1] -= 116.779\n",
    "    x[:, :, 2] -= 123.68\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_name = 'GTbbox_df.pickle'\n",
    "if os.path.exists('../data/'+file_name):\n",
    "    print ('Loading from file '+file_name)\n",
    "    GTbbox_df = pd.read_pickle('../data/'+file_name)\n",
    "else:\n",
    "    print ('Generating file '+file_name)       \n",
    "    GTbbox_df = pd.DataFrame(columns=['image_folder', 'image_file','crop_index','crop_class','xmin','ymin','xmax','ymax'])  \n",
    "\n",
    "    crop_classes=FISH_CLASSES[:]\n",
    "    crop_classes.remove('NoF')\n",
    "\n",
    "    for c in crop_classes:\n",
    "        print(c)\n",
    "        j = json.load(open('../data/fish/annos1/{}.json'.format(c), 'r'))\n",
    "        for l in j: \n",
    "            filename = l[\"filename\"]\n",
    "            head, image_file = os.path.split(filename)\n",
    "            basename, file_extension = os.path.splitext(image_file) \n",
    "            image = Image.open(TRAIN_DIR+c+'/'+image_file)\n",
    "            width_image, height_image = image.size\n",
    "            for i in range(len(l[\"annotations\"])):\n",
    "                a = l[\"annotations\"][i]\n",
    "                xmin = (a[\"x\"])\n",
    "                ymin = (a[\"y\"])\n",
    "                width = (a[\"width\"])\n",
    "                height = (a[\"height\"])\n",
    "                delta_width = p/(COLS-2*p)*width\n",
    "                delta_height = p/(ROWS-2*p)*height\n",
    "                xmin_expand = xmin-delta_width\n",
    "                ymin_expand = ymin-delta_height\n",
    "                xmax_expand = xmin+width+delta_width\n",
    "                ymax_expand = ymin+height+delta_height\n",
    "                assert max(xmin_expand,0)<min(xmax_expand,width_image)\n",
    "                assert max(ymin_expand,0)<min(ymax_expand,height_image)\n",
    "                GTbbox_df.loc[len(GTbbox_df)]=[c, image_file,i,a[\"class\"],max(xmin_expand,0),max(ymin_expand,0),min(xmax_expand,width_image),min(ymax_expand,height_image)]\n",
    "                if a[\"class\"] != c: print(GTbbox_df.tail(1))\n",
    "                    \n",
    "    GTbbox_df.to_pickle('../data/'+file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GTbbox_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_generator(datagen, df):\n",
    "    while 1:\n",
    "        batch_x = np.zeros((BATCHSIZE, ROWS, COLS, 3), dtype=K.floatx())\n",
    "        batch_y = np.zeros((BATCHSIZE, len(FISH_CLASSES)), dtype=K.floatx())\n",
    "        fn = lambda obj: obj.loc[np.random.choice(obj.index, size=nb_perClass, replace=False),:]\n",
    "        batch_df = df.groupby('crop_class', as_index=True).apply(fn)\n",
    "        i = 0\n",
    "        for index,row in batch_df.iterrows():\n",
    "            row = row.tolist()\n",
    "            image_file = os.path.join(row[0], row[1])\n",
    "            fish = row[3]\n",
    "            bbox = row[4:8]\n",
    "            cropped = load_img(TRAIN_DIR+image_file,bbox,target_size=(ROWS,COLS))\n",
    "            x = np.asarray(cropped, dtype=K.floatx())\n",
    "            x = datagen.random_transform(x)\n",
    "            x = preprocess_input(x)\n",
    "            batch_x[i] = x\n",
    "            batch_y[i,FISH_CLASSES.index(fish)] = 1\n",
    "            i += 1\n",
    "        yield (batch_x, batch_y)\n",
    "\n",
    "def test_generator(datagen, df):\n",
    "    while 1:\n",
    "        batch_x = np.zeros((BATCHSIZE, ROWS, COLS, 3), dtype=K.floatx())\n",
    "        #batch_y = np.zeros((BATCHSIZE, len(FISH_CLASSES)), dtype=K.floatx())\n",
    "        fn = lambda obj: obj.loc[np.random.choice(obj.index, size=nb_perClass, replace=False),:]\n",
    "        batch_df = df.groupby('crop_class', as_index=True).apply(fn)\n",
    "        i = 0\n",
    "        for index,row in batch_df.iterrows():\n",
    "            row = row.tolist()\n",
    "            image_file = row[1]\n",
    "            fish = row[3]\n",
    "            bbox = row[4:8]\n",
    "            cropped = load_img(TEST_DIR+image_file,bbox,target_size=(ROWS,COLS))\n",
    "            x = np.asarray(cropped, dtype=K.floatx())\n",
    "            x = datagen.random_transform(x)\n",
    "            x = preprocess_input(x)\n",
    "            batch_x[i] = x\n",
    "            # batch_y[i,FISH_CLASSES.index(fish)] = 1\n",
    "            i += 1\n",
    "        yield (batch_x)\n",
    "        \n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=180,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.1,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in the validation set\n",
    "df_valid = pd.read_csv('../image_validation_set.csv')\n",
    "valid_set = df_valid[df_valid['Validation']==1].file_name.tolist()\n",
    "#train data prepare\n",
    "# train_df, valid_df = train_test_split(GTbbox_df, test_size = 0.2, random_state=1986, stratify=GTbbox_df['crop_class'])\n",
    "train_df = GTbbox_df[~GTbbox_df['image_file'].isin(valid_set)]\n",
    "valid_df = GTbbox_df[GTbbox_df['image_file'].isin(valid_set)]\n",
    "\n",
    "nb_perClass = int(BATCHSIZE / len(FISH_CLASSES)) \n",
    "samples_per_epoch=BATCHSIZE*math.ceil(train_df.groupby('crop_class').size()['ALB']/nb_perClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if full == True:\n",
    "    train_df = GTbbox_df\n",
    "    samples_per_epoch=BATCHSIZE*math.ceil(train_df.groupby('crop_class').size()['ALB']/nb_perClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generator_test(gen, df, DIR):\n",
    "        batch_x = np.zeros((BATCHSIZE, ROWS, COLS, 3), dtype=K.floatx())\n",
    "        batch_y = np.zeros((BATCHSIZE, len(FISH_CLASSES)), dtype=K.floatx())\n",
    "        fn = lambda obj: obj.loc[np.random.choice(obj.index, size=nb_perClass, replace=False),:]\n",
    "        batch_df = df.groupby('crop_class', as_index=True).apply(fn)\n",
    "        i = 0\n",
    "        for index,row in batch_df.iterrows():\n",
    "            row = row.tolist()\n",
    "            image_file = os.path.join(row[0], row[1])\n",
    "            fish = row[3]\n",
    "            bbox = row[4:8]\n",
    "            cropped = load_img(DIR+image_file,bbox,target_size=(ROWS,COLS))\n",
    "        #return cropped\n",
    "        img = np.expand_dims(cropped,0)\n",
    "        aug_iter = gen.flow(img)\n",
    "        aug_imgs = [next(aug_iter)[0].astype(np.uint8) for i in range(8)]\n",
    "        return aug_imgs, cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aug_imgs, cropped = generator_test(train_datagen, df=train_df, DIR = TRAIN_DIR)\n",
    "# Augmented data\n",
    "plt.imshow(cropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Augmented data\n",
    "plots(aug_imgs, (20,7), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valid_df[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# validation_data (valid_x,valid_y)\n",
    "df_1 = valid_df[valid_df.crop_class != 'NoF']\n",
    "l = valid_df.groupby('crop_class').size()\n",
    "# l.pop('NoF')\n",
    "nb_NoF_valid = math.ceil(l.sum()/10)\n",
    "#df_2 = valid_df[valid_df.crop_class == 'NoF'].sample(n=nb_NoF_valid)\n",
    "#valid_df = pd.concat([df_1,df_2], axis=0)\n",
    "valid_x = np.zeros((valid_df.shape[0], ROWS, COLS, 3), dtype=K.floatx())\n",
    "valid_y = np.zeros((valid_df.shape[0], len(FISH_CLASSES)), dtype=K.floatx())\n",
    "i = 0\n",
    "for index,row in valid_df.iterrows():\n",
    "    row = row.tolist()\n",
    "    image_file = os.path.join(row[0], row[1])\n",
    "    fish = row[3]\n",
    "    bbox = row[4:8]\n",
    "    cropped = load_img(TRAIN_DIR+image_file,bbox,target_size=(ROWS,COLS))\n",
    "    x = np.asarray(cropped, dtype=K.floatx())\n",
    "    x = preprocess_input(x)\n",
    "    valid_x[i] = x\n",
    "    valid_y[i,FISH_CLASSES.index(fish)] = 1\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#callbacks\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')        \n",
    "\n",
    "model_checkpoint = ModelCheckpoint(filepath=CHECKPOINT_DIR+'weights.{epoch:03d}-{val_loss:.4f}.hdf5', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto')\n",
    "        \n",
    "learningrate_schedule = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, mode='auto', epsilon=0.001, cooldown=0, min_lr=0)\n",
    "\n",
    "# tensorboard = TensorBoard(log_dir=LOG_DIR, histogram_freq=0, write_graph=False, write_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Resnet50\n",
    "#top layer training\n",
    "\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "\n",
    "base_model = ResNet50(weights='imagenet', include_top=False)\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "#x = Flatten()(x)\n",
    "#x = Dense(256, init='glorot_normal', activation='relu')(x)\n",
    "#x = LeakyReLU(alpha=0.33)(x)\n",
    "x = Dropout(0.5)(x)\n",
    "#x = Dense(256, init='glorot_normal', activation='relu')(x)\n",
    "x = Dense(256, init='glorot_normal')(x)\n",
    "x = LeakyReLU(alpha=0.33)(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(len(FISH_CLASSES), init='glorot_normal', activation='softmax')(x)\n",
    "\n",
    "model = Model(input=base_model.input, output=predictions)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "optimizer = Adam(lr=LEARNINGRATE)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train the model on the new data for a few epochs\n",
    "model.fit_generator(train_generator(datagen=train_datagen, df=train_df), samples_per_epoch=samples_per_epoch, nb_epoch=12, verbose=1,\n",
    "                    callbacks=[early_stopping, model_checkpoint, learningrate_schedule],  # , tensorboard\n",
    "                    validation_data=(valid_x,valid_y), nb_worker=3, pickle_safe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Resnet50\n",
    "# fine tuning\n",
    "model_checkpoint = ModelCheckpoint(filepath=CHECKPOINT_DIR+'weights.{epoch:03d}-{val_loss:.4f}.hdf5', monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto')\n",
    "import glob\n",
    "start_layer = 38\n",
    "\n",
    "files = glob.glob(CHECKPOINT_DIR+'*')\n",
    "val_losses = [float(f.split('-')[-1][:-5]) for f in files]\n",
    "index = val_losses.index(min(val_losses))\n",
    "print('Loading model from checkpoints file ' + files[index])\n",
    "model = load_model(files[index])\n",
    "\n",
    "for layer in model.layers[:start_layer]:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[start_layer:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "optimizer = Adam(lr=1e-5)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit_generator(train_generator(datagen=train_datagen, df=train_df), samples_per_epoch=samples_per_epoch, nb_epoch=5, verbose=1,\n",
    "                    callbacks=[early_stopping, model_checkpoint, learningrate_schedule],  # , tensorboard\n",
    "                    validation_data=(valid_x,valid_y), nb_worker=1, pickle_safe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save at every epoch\n",
    "model_checkpoint = ModelCheckpoint(filepath=CHECKPOINT_DIR+'weights.{epoch:03d}-{val_loss:.4f}.hdf5', monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto')\n",
    "\n",
    "#resume training\n",
    "files = glob.glob(CHECKPOINT_DIR+'*')\n",
    "val_losses = [float(f.split('-')[-1][:-5]) for f in files]\n",
    "index = val_losses.index(min(val_losses))\n",
    "print('Loading model from checkpoints file ' + files[index])\n",
    "model = load_model(files[index])\n",
    "model.fit_generator(train_generator(datagen=train_datagen, df=train_df), samples_per_epoch=samples_per_epoch, nb_epoch=4, verbose=1,\n",
    "                    callbacks=[early_stopping, model_checkpoint, learningrate_schedule], # , tensorboard\n",
    "                    validation_data=(valid_x,valid_y), nb_worker=1, pickle_safe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GTbbox_df[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load up YOLO bounding boxes for each class\n",
    "import glob\n",
    "# all_files = glob.glob(os.path.join('../yolo_coords', \"*.txt\"))\n",
    "# allFiles = [f for f in all_files if 'FISH' in f]\n",
    "all_files = glob.glob(os.path.join('../darknet/results', \"*.txt\"))\n",
    "allFiles = [f for f in all_files if 'FISH640.' in f]\n",
    "frame = pd.DataFrame()\n",
    "list_ = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=None, sep = \" \", names = ['fname', 'proba', 'x0', 'y0', 'x1', 'y1'])\n",
    "    df['class'] = file_.split('_')[-1].split('.')[0]\n",
    "    list_.append(df)\n",
    "yolo_frame = pd.concat(list_)\n",
    "# Cut off the predictions on a probabilty\n",
    "yolo_frame = yolo_frame[yolo_frame['proba']>0.6]\n",
    "# Sort the predictions on the area \n",
    "yolo_frame['proba_max'] = df.groupby('fname')['proba'].transform('max')\n",
    "yolo_frame['area'] = (yolo_frame['x1']-yolo_frame['x0']) * (yolo_frame['y1']-yolo_frame['y0'])\n",
    "yolo_frame = yolo_frame.sort(['fname','area'], ascending=[1, 0]).reset_index(drop=True)\n",
    "yolo_frame = pd.concat([yolo_frame[(yolo_frame['proba']==yolo_frame['proba_max'])&(yolo_frame['proba_max']<0.80)],\n",
    "                       yolo_frame[yolo_frame['proba'] >= .80]], axis = 0)\n",
    "yolo_frame[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(yolo_frame.fname.unique())\n",
    "yolo_frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_name = 'GTbbox_test_df.pickle'\n",
    "if False: #os.path.exists('../data/'+file_name):\n",
    "    print ('Loading from file '+file_name)\n",
    "    GTbbox_test_df = pd.read_pickle('../data/'+file_name)\n",
    "else:\n",
    "    print ('Generating file '+file_name)       \n",
    "    GTbbox_test_df = pd.DataFrame(columns=['image_folder', 'image_file','crop_index','crop_class','xmin','ymin','xmax','ymax'])  \n",
    "    iddict = {}\n",
    "    for c in ['test']:\n",
    "        print(c)\n",
    "        for l in range(yolo_frame.shape[0]): \n",
    "            image_file, proba, xmin, ymin, xmax, ymax, fish_class, area = yolo_frame.iloc[l].values.tolist()    \n",
    "            if image_file in iddict:\n",
    "                iddict[image_file] += 1\n",
    "            else:\n",
    "                iddict[image_file] = 0\n",
    "            image = Image.open(TEST_DIR+c+'/'+image_file+'.jpg')\n",
    "            width_image, height_image = image.size\n",
    "            width = xmax - xmin\n",
    "            height = ymax - ymin\n",
    "            delta_width = p/(COLS-2*p)*width\n",
    "            delta_height = p/(ROWS-2*p)*height\n",
    "            xmin_expand = xmin-delta_width\n",
    "            ymin_expand = ymin-delta_height\n",
    "            xmax_expand = xmin+width+delta_width\n",
    "            ymax_expand = ymin+height+delta_height\n",
    "            assert max(xmin_expand,0)<min(xmax_expand,width_image)\n",
    "            assert max(ymin_expand,0)<min(ymax_expand,height_image)\n",
    "            GTbbox_test_df.loc[len(GTbbox_test_df)] = [c, image_file+'.jpg', iddict[image_file],fish_class,max(xmin_expand,0),max(ymin_expand,0),min(xmax_expand,width_image),min(ymax_expand,height_image)]                    \n",
    "    GTbbox_test_df = GTbbox_test_df.sort(['image_file','crop_index']).reset_index(drop=True)\n",
    "    GTbbox_test_df.to_pickle('../data/'+file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GTbbox_test_df[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#image_file, proba, x0, y0, x1, y1, fish_class = yolo_frame.iloc[0].values.tolist()    \n",
    "#image = Image.open(TEST_DIR+c+'/'+image_file+'.jpg')\n",
    "#image\n",
    "aug_imgs, cropped = generator_test(train_datagen, df=GTbbox_test_df, DIR = '../data/fish/')\n",
    "# Augmented data\n",
    "plt.imshow(cropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Augmented data\n",
    "plots(aug_imgs, (20,7), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GTbbox_test_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_generator(df, datagen = None, batch_size = BATCHSIZE):\n",
    "    n = df.shape[0]\n",
    "    batch_index = 0\n",
    "    while 1:\n",
    "        current_index = batch_index * batch_size\n",
    "        if n >= current_index + batch_size:\n",
    "            current_batch_size = batch_size\n",
    "            batch_index += 1    \n",
    "        else:\n",
    "            current_batch_size = n - current_index\n",
    "            batch_index = 0        \n",
    "        batch_df = df[current_index:current_index+current_batch_size]\n",
    "        batch_x = np.zeros((batch_df.shape[0], ROWS, COLS, 3), dtype=K.floatx())\n",
    "        i = 0\n",
    "        for index,row in batch_df.iterrows():\n",
    "            image_file = row['image_file']\n",
    "            bbox = [row['xmin'],row['ymin'],row['xmax'],row['ymax']]\n",
    "            cropped = load_img(TEST_DIR+image_file,bbox,target_size=(ROWS,COLS))\n",
    "            x = np.asarray(cropped, dtype=K.floatx())\n",
    "            if datagen is not None: x = datagen.random_transform(x)            \n",
    "            x = preprocess_input(x)\n",
    "            batch_x[i] = x\n",
    "            i += 1\n",
    "        if batch_index%50 == 0: print(batch_index)\n",
    "        yield(batch_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "files = glob.glob(CHECKPOINT_DIR+'*')\n",
    "val_losses = [float(f.split('-')[-1][:-5]) for f in files]\n",
    "min_id = np.array(val_losses).argsort()[:bags].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loop the the lowest val losses and get a prediction for each\n",
    "test_preds_ls = []\n",
    "for index in min_id:\n",
    "    index = val_losses.index(min(val_losses))\n",
    "    print('Loading model from checkpoints file ' + files[index])\n",
    "    test_model = load_model(files[index])\n",
    "    test_model_name = files[index].split('/')[-2][-1:]+'_'+files[index].split('/')[-1]\n",
    "    test_preds_ls.append(test_model.predict_generator(test_generator(df=GTbbox_test_df, datagen = train_datagen), \n",
    "                                         val_samples=GTbbox_test_df.shape[0])) \n",
    "    del test_model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_preds = sum(test_preds_ls)/len(test_preds_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GTbbox_test_df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = ['NoF', 'ALB', 'BET', 'DOL', 'LAG', 'OTHER', 'SHARK', 'YFT']\n",
    "yolo_pred_df = pd.DataFrame(test_preds, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yolo_pred_df['image_file'] = GTbbox_test_df.image_file\n",
    "yolo_pred_df['crop_index'] = GTbbox_test_df.crop_index\n",
    "yolo_pred_df[yolo_pred_df['crop_index']<2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yolo_pred_df = yolo_pred_df.groupby(['image_file'], as_index=False).mean().reset_index(drop=True)\n",
    "yolo_pred_df.drop('crop_index', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "timestr = time.strftime(\"%Y%m%d\")\n",
    "if full:\n",
    "    subm_name = '../sub/subm_full_resnet_box640_cut0.7_' + timestr + '.csv' #'.csv.gz'\n",
    "else:\n",
    "    subm_name = '../sub/subm_part_resnet_box640_cut0.7_' + timestr + '.csv' #'.csv.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yolo_pred_df.to_csv(subm_name, index=False)#, compression='gzip')\n",
    "FileLink(subm_name)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
