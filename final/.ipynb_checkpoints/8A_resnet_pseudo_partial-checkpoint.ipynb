{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Leveraged from 4A_resnet_cropsq_partial.py\n",
    "# Read in Libraries\n",
    "from __future__ import division, print_function\n",
    "from logbook import Logger, StreamHandler\n",
    "import sys\n",
    "StreamHandler(sys.stdout).push_application()\n",
    "log = Logger('Logbook')\n",
    "import shutil, csv, time\n",
    "timestr = time.strftime(\"%Y%m%d\")\n",
    "import ujson as json\n",
    "\n",
    "import os, random, glob, pickle, collections, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ujson as json\n",
    "from PIL import Image\n",
    "import gc\n",
    "import glob\n",
    "import shutil, csv, time\n",
    "from distutils.dir_util import copy_tree\n",
    "\n",
    "import utils; reload(utils)\n",
    "from utils import *\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "from keras.models import Sequential, Model, load_model, model_from_json\n",
    "from keras.layers import GlobalAveragePooling2D, Flatten, Dropout, Dense, LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import image\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set Parameters and check files\n",
    "refresh_directories = True\n",
    "TRAIN_DIR = '../data/fish/train-all/'\n",
    "TEST_DIR =  '../data/fish/test/'\n",
    "CHECKPOINT_DIR = './checkpoints/checkpoint08A/'\n",
    "path = \"../data/fish/\"\n",
    "LOG_DIR = './logs'\n",
    "FISH_CLASSES = ['NoF', 'ALB', 'BET', 'DOL', 'LAG', 'OTHER', 'SHARK', 'YFT']\n",
    "CONF_THRESH = 0.8\n",
    "ROWS = 224\n",
    "COLS = 224\n",
    "BATCHSIZE = 32 # 256 #64\n",
    "LEARNINGRATE = 1e-4\n",
    "BG_THRESH_HI = 0.3\n",
    "BG_THRESH_LO = 0.1\n",
    "bags = 5\n",
    "learn_round = 2\n",
    "p=16\n",
    "full = True\n",
    "sub_cutoff = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-04-01 19:23:58.394848] INFO: Logbook: Create directory structure and validation files\n"
     ]
    }
   ],
   "source": [
    "def accuracyfunc(y_act, y_pred):\n",
    "    return metrics.accuracy_score(np.argmax(y_act, axis=1), np.argmax(y_pred, axis=1))\n",
    "    \n",
    "def refresh_directory_structure(name, sub_dirs):\n",
    "    gdir = os.path.join(path, name)\n",
    "    if os.path.exists(gdir):\n",
    "        shutil.rmtree(gdir)\n",
    "    os.makedirs(gdir)\n",
    "    for sub_dir in sub_dirs:\n",
    "        os.makedirs(os.path.join(gdir, sub_dir))\n",
    "        \n",
    "# Create the test and valid directory\n",
    "if refresh_directories:\n",
    "    log.info('Create directory structure and validation files')\n",
    "    sub_dirs = os.listdir(os.path.join(path, 'train-all'))\n",
    "    if '.DS_Store' in sub_dirs: sub_dirs.remove('.DS_Store')\n",
    "    refresh_directory_structure('pseudoresnet/train', sub_dirs)\n",
    "    refresh_directory_structure('pseudoresnet/valid', sub_dirs)\n",
    "    refresh_directory_structure('pseudoresnet/test', ['test'])\n",
    "    for c,row in enumerate(csv.DictReader(open('../image_validation_set.csv'))):\n",
    "        value = 'pseudoresnet/valid' if row['Validation'] == '1' else 'pseudoresnet/train'\n",
    "        name_from = os.path.join(path, 'train-all', row['SubDirectory'], row['file_name'])\n",
    "        name_to   = os.path.join(path, value, row['SubDirectory'], row['file_name'])\n",
    "        shutil.copyfile(name_from, name_to)   \n",
    "    copy_tree(os.path.join(path, 'test'), os.path.join(path, 'pseudoresnet/test'))\n",
    "    copy_tree(os.path.join(path, 'test'), os.path.join(path, 'pseudoresnet/train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_img(path, bbox, target_size=None):\n",
    "    img = Image.open(path)\n",
    "    imsize = Image.open(path).size\n",
    "    height, width = bbox[2]-bbox[0], bbox[3]-bbox[1]\n",
    "    length = max(height, width)\n",
    "    # Make it square\n",
    "    dim = [width, height]\n",
    "    for i in range(2):\n",
    "        offset = length - dim[0+i]\n",
    "        if bbox[0+i]+length+(offset/2) > imsize[0+i]:\n",
    "            bbox[0+i] = bbox[2+i] - length + (offset/2)\n",
    "            bbox[2+i] = bbox[2+i] + (offset/2)\n",
    "        else:\n",
    "            bbox[2+i] = bbox[0+i] + length\n",
    "        bbox[0+i] -= length*0.05\n",
    "        bbox[2+i] += length*0.05\n",
    "\n",
    "    img = img.convert('RGB')\n",
    "    cropped = img.crop((bbox[0],bbox[1],bbox[2],bbox[3]))\n",
    "    if target_size:\n",
    "        cropped = cropped.resize((target_size[1], target_size[0]))\n",
    "    if height < width:\n",
    "        cropped = cropped.rotate(-90)\n",
    "    return cropped\n",
    "\n",
    "def preprocess_input(x):\n",
    "    #resnet50 image preprocessing\n",
    "    # 'RGB'->'BGR'\n",
    "    x = x[:, :, ::-1]\n",
    "    x[:, :, 0] -= 105\n",
    "    x[:, :, 1] -= 115\n",
    "    x[:, :, 2] -= 123\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from file GTbbox_df.pickle\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4371, 8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = 'GTbbox_df.pickle'\n",
    "if os.path.exists('../data/'+file_name):\n",
    "    print ('Loading from file '+file_name)\n",
    "    GTbbox_df = pd.read_pickle('../data/'+file_name)\n",
    "else:\n",
    "    print ('Generating file '+file_name)\n",
    "    GTbbox_df = pd.DataFrame(columns=['image_folder', 'image_file','crop_index','crop_class','xmin','ymin','xmax','ymax'])\n",
    "\n",
    "    crop_classes=FISH_CLASSES[:]\n",
    "    crop_classes.remove('NoF')\n",
    "\n",
    "    for c in crop_classes:\n",
    "        print(c)\n",
    "        j = json.load(open('../data/fish/annos1/{}.json'.format(c), 'r'))\n",
    "        for l in j:\n",
    "            filename = l[\"filename\"]\n",
    "            head, image_file = os.path.split(filename)\n",
    "            basename, file_extension = os.path.splitext(image_file)\n",
    "            image = Image.open(TRAIN_DIR+c+'/'+image_file)\n",
    "            width_image, height_image = image.size\n",
    "            for i in range(len(l[\"annotations\"])):\n",
    "                a = l[\"annotations\"][i]\n",
    "                xmin = (a[\"x\"])\n",
    "                ymin = (a[\"y\"])\n",
    "                width = (a[\"width\"])\n",
    "                height = (a[\"height\"])\n",
    "                delta_width = p/(COLS-2*p)*width\n",
    "                delta_height = p/(ROWS-2*p)*height\n",
    "                xmin_expand = xmin-delta_width\n",
    "                ymin_expand = ymin-delta_height\n",
    "                xmax_expand = xmin+width+delta_width\n",
    "                ymax_expand = ymin+height+delta_height\n",
    "                assert max(xmin_expand,0)<min(xmax_expand,width_image)\n",
    "                assert max(ymin_expand,0)<min(ymax_expand,height_image)\n",
    "                GTbbox_df.loc[len(GTbbox_df)]=[c, image_file,i,a[\"class\"],max(xmin_expand,0),max(ymin_expand,0),min(xmax_expand,width_image),min(ymax_expand,height_image)]\n",
    "                if a[\"class\"] != c: print(GTbbox_df.tail(1))\n",
    "\n",
    "    GTbbox_df.to_pickle('../data/'+file_name)\n",
    "GTbbox_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:16: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "# Load up YOLO bounding boxes for each class\n",
    "import glob\n",
    "all_files = glob.glob(os.path.join('../darknet/results', \"*.txt\"))\n",
    "allFiles = [f for f in all_files if 'test_FISH.txt' in f]\n",
    "frame = pd.DataFrame()\n",
    "list_ = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=None, sep = \" \", names = ['fname', 'proba', 'x0', 'y0', 'x1', 'y1'])\n",
    "    df['class'] = file_.split('_')[-1].split('.')[0]\n",
    "    list_.append(df)\n",
    "yolo_frame = pd.concat(list_)\n",
    "# Cut off the predictions on a probabilty\n",
    "yolo_frame = yolo_frame[yolo_frame['proba']>0.7]\n",
    "# Sort the predictions on the area \n",
    "yolo_frame['area'] = (yolo_frame['x1']-yolo_frame['x0']) * (yolo_frame['y1']-yolo_frame['y0'])\n",
    "yolo_frame = yolo_frame.sort(['fname','area'], ascending=[1, 0]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating file GTbbox_test_df.pickle\n",
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:30: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "688"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = 'GTbbox_test_df.pickle'\n",
    "if False: #os.path.exists('../data/'+file_name):\n",
    "    print ('Loading from file '+file_name)\n",
    "    GTbbox_test_df = pd.read_pickle('../data/'+file_name)\n",
    "else:\n",
    "    print ('Generating file '+file_name)\n",
    "    GTbbox_test_df = pd.DataFrame(columns=['image_folder', 'image_file','crop_index','crop_class','xmin','ymin','xmax','ymax'])\n",
    "    iddict = {}\n",
    "    for c in ['test']:\n",
    "        print(c)\n",
    "        for l in range(yolo_frame.shape[0]):\n",
    "            image_file, proba, xmin, ymin, xmax, ymax, fish_class, area = yolo_frame.iloc[l].values.tolist()\n",
    "            if image_file in iddict:\n",
    "                iddict[image_file] += 1\n",
    "            else:\n",
    "                iddict[image_file] = 0\n",
    "            image = Image.open(TEST_DIR+c+'/'+image_file+'.jpg')\n",
    "            width_image, height_image = image.size\n",
    "            width = xmax - xmin\n",
    "            height = ymax - ymin\n",
    "            delta_width = p/(COLS-2*p)*width\n",
    "            delta_height = p/(ROWS-2*p)*height\n",
    "            xmin_expand = xmin-delta_width\n",
    "            ymin_expand = ymin-delta_height\n",
    "            xmax_expand = xmin+width+delta_width\n",
    "            ymax_expand = ymin+height+delta_height\n",
    "            assert max(xmin_expand,0)<min(xmax_expand,width_image)\n",
    "            assert max(ymin_expand,0)<min(ymax_expand,height_image)\n",
    "            GTbbox_test_df.loc[len(GTbbox_test_df)] = [c, image_file+'.jpg', iddict[image_file],fish_class,max(xmin_expand,0),max(ymin_expand,0),min(xmax_expand,width_image),min(ymax_expand,height_image)]  \n",
    "    GTbbox_test_df = GTbbox_test_df.sort(['image_file','crop_index']).reset_index(drop=True)\n",
    "    GTbbox_test_df.to_pickle('../data/'+file_name)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>img_00005.jpg</td>\n",
       "      <td>NoF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>img_00007.jpg</td>\n",
       "      <td>YFT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>img_00009.jpg</td>\n",
       "      <td>ALB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>img_00030.jpg</td>\n",
       "      <td>NoF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>img_00053.jpg</td>\n",
       "      <td>ALB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           image Class\n",
       "0  img_00005.jpg   NoF\n",
       "1  img_00007.jpg   YFT\n",
       "2  img_00009.jpg   ALB\n",
       "3  img_00030.jpg   NoF\n",
       "4  img_00053.jpg   ALB"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use our best some to idenify high confidence test images\n",
    "best_sub = pd.read_csv(\"../sub/final-round1-input-pseudo.csv\")\n",
    "hiconf_test = best_sub[best_sub.drop(['image'], axis=1).apply(np.max, axis=1)>sub_cutoff].reset_index(drop=True)\n",
    "hiconf_test['Class'] = hiconf_test.drop(['image'], axis=1).idxmax(axis = 1)\n",
    "hiconf_test = hiconf_test[['image', 'Class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_folder</th>\n",
       "      <th>image_file</th>\n",
       "      <th>crop_index</th>\n",
       "      <th>crop_class</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>test</td>\n",
       "      <td>img_07895.jpg</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ALB</td>\n",
       "      <td>1005.277060</td>\n",
       "      <td>224.534027</td>\n",
       "      <td>1205.163859</td>\n",
       "      <td>360.591827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>test</td>\n",
       "      <td>img_07905.jpg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ALB</td>\n",
       "      <td>560.475189</td>\n",
       "      <td>343.951345</td>\n",
       "      <td>636.750153</td>\n",
       "      <td>538.266429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>test</td>\n",
       "      <td>img_07906.jpg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>YFT</td>\n",
       "      <td>559.614573</td>\n",
       "      <td>502.235352</td>\n",
       "      <td>946.565236</td>\n",
       "      <td>789.921509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>test</td>\n",
       "      <td>img_07908.jpg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ALB</td>\n",
       "      <td>680.336136</td>\n",
       "      <td>126.793386</td>\n",
       "      <td>1075.019028</td>\n",
       "      <td>281.850962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>test</td>\n",
       "      <td>img_07910.jpg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ALB</td>\n",
       "      <td>695.727804</td>\n",
       "      <td>306.770034</td>\n",
       "      <td>820.306498</td>\n",
       "      <td>526.803269</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    image_folder     image_file  crop_index crop_class         xmin  \\\n",
       "607         test  img_07895.jpg         1.0        ALB  1005.277060   \n",
       "608         test  img_07905.jpg         0.0        ALB   560.475189   \n",
       "609         test  img_07906.jpg         0.0        YFT   559.614573   \n",
       "610         test  img_07908.jpg         0.0        ALB   680.336136   \n",
       "611         test  img_07910.jpg         0.0        ALB   695.727804   \n",
       "\n",
       "           ymin         xmax        ymax  \n",
       "607  224.534027  1205.163859  360.591827  \n",
       "608  343.951345   636.750153  538.266429  \n",
       "609  502.235352   946.565236  789.921509  \n",
       "610  126.793386  1075.019028  281.850962  \n",
       "611  306.770034   820.306498  526.803269  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the pseudo images and add to main set\n",
    "GTbbox_pseudo_df = pd.merge(GTbbox_test_df, hiconf_test, left_on='image_file', right_on='image', how='inner')\n",
    "GTbbox_pseudo_df['crop_class'] = GTbbox_pseudo_df['Class']\n",
    "GTbbox_pseudo_df.drop(['image', 'Class'], axis=1, inplace=True)\n",
    "GTbbox_df = pd.concat([GTbbox_df, GTbbox_pseudo_df], axis=0)\n",
    "GTbbox_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_generator(datagen, df):\n",
    "    while 1:\n",
    "        batch_x = np.zeros((BATCHSIZE, ROWS, COLS, 3), dtype=K.floatx())\n",
    "        batch_y = np.zeros((BATCHSIZE, len(FISH_CLASSES)), dtype=K.floatx())\n",
    "        fn = lambda obj: obj.loc[np.random.choice(obj.index, size=nb_perClass, replace=False),:]\n",
    "        batch_df = df.groupby('crop_class', as_index=True).apply(fn)\n",
    "        i = 0\n",
    "        for index,row in batch_df.iterrows():\n",
    "            row = row.tolist()\n",
    "            image_file = os.path.join(row[0], row[1])\n",
    "            fish = row[3]\n",
    "            bbox = row[4:8]\n",
    "            cropped = load_img(TRAIN_DIR+image_file,bbox,target_size=(ROWS,COLS))\n",
    "            x = np.asarray(cropped, dtype=K.floatx())\n",
    "            x = datagen.random_transform(x)\n",
    "            x = preprocess_input(x)\n",
    "            batch_x[i] = x\n",
    "            batch_y[i,FISH_CLASSES.index(fish)] = 1\n",
    "            i += 1\n",
    "        yield (batch_x, batch_y)\n",
    "\n",
    "def test_generator(datagen, df):\n",
    "    while 1:\n",
    "        batch_x = np.zeros((BATCHSIZE, ROWS, COLS, 3), dtype=K.floatx())\n",
    "        #batch_y = np.zeros((BATCHSIZE, len(FISH_CLASSES)), dtype=K.floatx())\n",
    "        fn = lambda obj: obj.loc[np.random.choice(obj.index, size=nb_perClass, replace=False),:]\n",
    "        batch_df = df.groupby('crop_class', as_index=True).apply(fn)\n",
    "        i = 0\n",
    "        for index,row in batch_df.iterrows():\n",
    "            row = row.tolist()\n",
    "            image_file = row[1]\n",
    "            fish = row[3]\n",
    "            bbox = row[4:8]\n",
    "            cropped = load_img(TEST_DIR+image_file,bbox,target_size=(ROWS,COLS))\n",
    "            x = np.asarray(cropped, dtype=K.floatx())\n",
    "            x = datagen.random_transform(x)\n",
    "            x = preprocess_input(x)\n",
    "            batch_x[i] = x\n",
    "            # batch_y[i,FISH_CLASSES.index(fish)] = 1\n",
    "            i += 1\n",
    "        yield (batch_x)\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=180,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.1,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in the validation set\n",
    "df_valid = pd.read_csv('../image_validation_set.csv')\n",
    "valid_set = df_valid[df_valid['Validation']==1].file_name.tolist()\n",
    "#train data prepare\n",
    "# train_df, valid_df = train_test_split(GTbbox_df, test_size = 0.2, random_state=1986, stratify=GTbbox_df['crop_class'])\n",
    "train_df = GTbbox_df[~GTbbox_df['image_file'].isin(valid_set)]\n",
    "valid_df = GTbbox_df[GTbbox_df['image_file'].isin(valid_set)]\n",
    "\n",
    "nb_perClass = int(BATCHSIZE / len(FISH_CLASSES))\n",
    "samples_per_epoch=BATCHSIZE*math.ceil(train_df.groupby('crop_class').size()['ALB']/nb_perClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if full == True:\n",
    "    train_df = GTbbox_df\n",
    "    samples_per_epoch=BATCHSIZE*math.ceil(train_df.groupby('crop_class').size()['ALB']/nb_perClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator_test(gen, df, DIR):\n",
    "        batch_x = np.zeros((BATCHSIZE, ROWS, COLS, 3), dtype=K.floatx())\n",
    "        batch_y = np.zeros((BATCHSIZE, len(FISH_CLASSES)), dtype=K.floatx())\n",
    "        fn = lambda obj: obj.loc[np.random.choice(obj.index, size=nb_perClass, replace=False),:]\n",
    "        batch_df = df.groupby('crop_class', as_index=True).apply(fn)\n",
    "        i = 0\n",
    "        for index,row in batch_df.iterrows():\n",
    "            row = row.tolist()\n",
    "            image_file = os.path.join(row[0], row[1])\n",
    "            fish = row[3]\n",
    "            bbox = row[4:8]\n",
    "            cropped = load_img(DIR+image_file,bbox,target_size=(ROWS,COLS))\n",
    "        #return cropped\n",
    "        img = np.expand_dims(cropped,0)\n",
    "        aug_iter = gen.flow(img)\n",
    "        aug_imgs = [next(aug_iter)[0].astype(np.uint8) for i in range(8)]\n",
    "        return aug_imgs, cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# validation_data (valid_x,valid_y)\n",
    "df_1 = valid_df[valid_df.crop_class != 'NoF']\n",
    "l = valid_df.groupby('crop_class').size()\n",
    "nb_NoF_valid = math.ceil(l.sum()/10)\n",
    "valid_x = np.zeros((valid_df.shape[0], ROWS, COLS, 3), dtype=K.floatx())\n",
    "valid_y = np.zeros((valid_df.shape[0], len(FISH_CLASSES)), dtype=K.floatx())\n",
    "i = 0\n",
    "for index,row in valid_df.iterrows():\n",
    "    row = row.tolist()\n",
    "    image_file = os.path.join(row[0], row[1])\n",
    "    fish = row[3]\n",
    "    bbox = row[4:8]\n",
    "    cropped = load_img(TRAIN_DIR+image_file,bbox,target_size=(ROWS,COLS))\n",
    "    x = np.asarray(cropped, dtype=K.floatx())\n",
    "    x = preprocess_input(x)\n",
    "    valid_x[i] = x\n",
    "    valid_y[i,FISH_CLASSES.index(fish)] = 1\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')\n",
    "model_checkpoint = ModelCheckpoint(filepath=CHECKPOINT_DIR+'weights.{epoch:03d}-{val_loss:.4f}.hdf5', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto')\n",
    "learningrate_schedule = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, mode='auto', epsilon=0.001, cooldown=0, min_lr=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Resnet50\n",
    "#top layer training\n",
    "\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "\n",
    "base_model = ResNet50(weights='imagenet', include_top=False)\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(256, init='glorot_normal')(x)\n",
    "x = LeakyReLU(alpha=0.33)(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(len(FISH_CLASSES), init='glorot_normal', activation='softmax')(x)\n",
    "\n",
    "model = Model(input=base_model.input, output=predictions)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "optimizer = Adam(lr=LEARNINGRATE)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train the model on the new data for a few epochs\n",
    "model.fit_generator(train_generator(datagen=train_datagen, df=train_df), samples_per_epoch=samples_per_epoch, nb_epoch=12, verbose=1,\n",
    "                    callbacks=[early_stopping, model_checkpoint, learningrate_schedule],  # , tensorboard\n",
    "                    validation_data=(valid_x,valid_y), nb_worker=3, pickle_safe=True)\n",
    "\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Resnet50\n",
    "# fine tuning\n",
    "model_checkpoint = ModelCheckpoint(filepath=CHECKPOINT_DIR+'weights.{epoch:03d}-{val_loss:.4f}.hdf5', monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto')\n",
    "import glob\n",
    "start_layer = 38\n",
    "\n",
    "files = glob.glob(CHECKPOINT_DIR+'*')\n",
    "val_losses = [float(f.split('-')[-1][:-5]) for f in files]\n",
    "index = val_losses.index(min(val_losses))\n",
    "print('Loading model from checkpoints file ' + files[index])\n",
    "model = load_model(files[index])\n",
    "\n",
    "for layer in model.layers[:start_layer]:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[start_layer:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "optimizer = Adam(lr=1e-5)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit_generator(train_generator(datagen=train_datagen, df=train_df), samples_per_epoch=samples_per_epoch, nb_epoch=5, verbose=1,\n",
    "                    callbacks=[early_stopping, model_checkpoint, learningrate_schedule],  # , tensorboard\n",
    "                    validation_data=(valid_x,valid_y), nb_worker=1, pickle_safe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save at every epoch\n",
    "model_checkpoint = ModelCheckpoint(filepath=CHECKPOINT_DIR+'weights.{epoch:03d}-{val_loss:.4f}.hdf5', monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto')\n",
    "\n",
    "#resume training\n",
    "files = glob.glob(CHECKPOINT_DIR+'*')\n",
    "val_losses = [float(f.split('-')[-1][:-5]) for f in files]\n",
    "index = val_losses.index(min(val_losses))\n",
    "print('Loading model from checkpoints file ' + files[index])\n",
    "model = load_model(files[index])\n",
    "model.fit_generator(train_generator(datagen=train_datagen, df=train_df), samples_per_epoch=samples_per_epoch, nb_epoch=4, verbose=1,\n",
    "                    callbacks=[early_stopping, model_checkpoint, learningrate_schedule], # , tensorboard\n",
    "                    validation_data=(valid_x,valid_y), nb_worker=1, pickle_safe=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:18: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "# Load up YOLO bounding boxes for each class\n",
    "import glob\n",
    "# all_files = glob.glob(os.path.join('../yolo_coords', \"*.txt\"))\n",
    "# allFiles = [f for f in all_files if 'FISH' in f]\n",
    "all_files = glob.glob(os.path.join('../darknet/results', \"*.txt\"))\n",
    "allFiles = [f for f in all_files if 'test_FISH.txt' in f]\n",
    "frame = pd.DataFrame()\n",
    "list_ = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=None, sep = \" \", names = ['fname', 'proba', 'x0', 'y0', 'x1', 'y1'])\n",
    "    df['class'] = file_.split('_')[-1].split('.')[0]\n",
    "    list_.append(df)\n",
    "yolo_frame = pd.concat(list_)\n",
    "# Cut off the predictions on a probabilty\n",
    "yolo_frame = yolo_frame[yolo_frame['proba']>0.7]\n",
    "# Sort the predictions on the area \n",
    "yolo_frame['area'] = (yolo_frame['x1']-yolo_frame['x0']) * (yolo_frame['y1']-yolo_frame['y0'])\n",
    "yolo_frame = yolo_frame.sort(['fname','area'], ascending=[1, 0]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating file GTbbox_test_df.pickle\n",
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:30: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "520"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = 'GTbbox_test_df.pickle'\n",
    "if False: #os.path.exists('../data/'+file_name):\n",
    "    print ('Loading from file '+file_name)\n",
    "    GTbbox_test_df = pd.read_pickle('../data/'+file_name)\n",
    "else:\n",
    "    print ('Generating file '+file_name)\n",
    "    GTbbox_test_df = pd.DataFrame(columns=['image_folder', 'image_file','crop_index','crop_class','xmin','ymin','xmax','ymax'])\n",
    "    iddict = {}\n",
    "    for c in ['test']:\n",
    "        print(c)\n",
    "        for l in range(yolo_frame.shape[0]):\n",
    "            image_file, proba, xmin, ymin, xmax, ymax, fish_class, area = yolo_frame.iloc[l].values.tolist()\n",
    "            if image_file in iddict:\n",
    "                iddict[image_file] += 1\n",
    "            else:\n",
    "                iddict[image_file] = 0\n",
    "            image = Image.open(TEST_DIR+c+'/'+image_file+'.jpg')\n",
    "            width_image, height_image = image.size\n",
    "            width = xmax - xmin\n",
    "            height = ymax - ymin\n",
    "            delta_width = p/(COLS-2*p)*width\n",
    "            delta_height = p/(ROWS-2*p)*height\n",
    "            xmin_expand = xmin-delta_width\n",
    "            ymin_expand = ymin-delta_height\n",
    "            xmax_expand = xmin+width+delta_width\n",
    "            ymax_expand = ymin+height+delta_height\n",
    "            assert max(xmin_expand,0)<min(xmax_expand,width_image)\n",
    "            assert max(ymin_expand,0)<min(ymax_expand,height_image)\n",
    "            GTbbox_test_df.loc[len(GTbbox_test_df)] = [c, image_file+'.jpg', iddict[image_file],fish_class,max(xmin_expand,0),max(ymin_expand,0),min(xmax_expand,width_image),min(ymax_expand,height_image)]  \n",
    "    GTbbox_test_df = GTbbox_test_df.sort(['image_file','crop_index']).reset_index(drop=True)\n",
    "    GTbbox_test_df.to_pickle('../data/'+file_name)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_generator(df, datagen = None, batch_size = BATCHSIZE):\n",
    "    n = df.shape[0]\n",
    "    batch_index = 0\n",
    "    while 1:\n",
    "        current_index = batch_index * batch_size\n",
    "        if n >= current_index + batch_size:\n",
    "            current_batch_size = batch_size\n",
    "            batch_index += 1\n",
    "        else:\n",
    "            current_batch_size = n - current_index\n",
    "            batch_index = 0\n",
    "        batch_df = df[current_index:current_index+current_batch_size]\n",
    "        batch_x = np.zeros((batch_df.shape[0], ROWS, COLS, 3), dtype=K.floatx())\n",
    "        i = 0\n",
    "        for index,row in batch_df.iterrows():\n",
    "            image_file = row['image_file']\n",
    "            bbox = [row['xmin'],row['ymin'],row['xmax'],row['ymax']]\n",
    "            cropped = load_img(TEST_DIR+image_file,bbox,target_size=(ROWS,COLS))\n",
    "            x = np.asarray(cropped, dtype=K.floatx())\n",
    "            if datagen is not None: x = datagen.random_transform(x)\n",
    "            x = preprocess_input(x)\n",
    "            batch_x[i] = x\n",
    "            i += 1\n",
    "        if batch_index%50 == 0: print(batch_index)\n",
    "        yield(batch_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "files = glob.glob(CHECKPOINT_DIR+'*')\n",
    "val_losses = [float(f.split('-')[-1][:-5]) for f in files]\n",
    "min_id = np.array(val_losses).argsort()[:bags].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loop the the lowest val losses and get a prediction for each\n",
    "test_preds_ls = []\n",
    "for index in min_id:\n",
    "    index = val_losses.index(min(val_losses))\n",
    "    print('Loading model from checkpoints file ' + files[index])\n",
    "    test_model = load_model(files[index])\n",
    "    test_model_name = files[index].split('/')[-2][-1:]+'_'+files[index].split('/')[-1]\n",
    "    test_preds_ls.append(test_model.predict_generator(test_generator(df=GTbbox_test_df, datagen = train_datagen),\n",
    "                                         val_samples=GTbbox_test_df.shape[0]))\n",
    "    del test_model\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "test_preds = sum(test_preds_ls)/len(test_preds_ls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = ['NoF', 'ALB', 'BET', 'DOL', 'LAG', 'OTHER', 'SHARK', 'YFT']\n",
    "yolo_pred_df = pd.DataFrame(test_preds, columns=columns)\n",
    "\n",
    "\n",
    "\n",
    "yolo_pred_df['image_file'] = GTbbox_test_df.image_file\n",
    "yolo_pred_df['crop_index'] = GTbbox_test_df.crop_index\n",
    "yolo_pred_df[yolo_pred_df['crop_index']<2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yolo_pred_df = yolo_pred_df.groupby(['image_file'], as_index=False).mean().reset_index(drop=True)\n",
    "yolo_pred_df.drop('crop_index', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "timestr = time.strftime(\"%Y%m%d\")\n",
    "if full:\n",
    "    subm_name = '../sub/subm_full_convsq_resnet_4A.csv'\n",
    "else:\n",
    "    subm_name = '../sub/subm_part_convsq_resnet_4A.csv'\n",
    "\n",
    "\n",
    "\n",
    "yolo_pred_df.to_csv(subm_name, index=False)#, compression='gzip')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
