{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-02-09 23:22:06.691323] INFO: Logbook: Set Paramters\n",
      "[2017-02-09 23:22:06.692352] INFO: Logbook: Get VGG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n",
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-02-09 23:22:09.926933] INFO: Logbook: Create VGG\n",
      "Found 3086 images belonging to 8 classes.\n",
      "Found 691 images belonging to 8 classes.\n",
      "Found 1000 images belonging to 1 classes.\n",
      "[2017-02-09 23:22:10.456098] INFO: Logbook: Read filenames\n"
     ]
    }
   ],
   "source": [
    "# Read in Libraries\n",
    "from __future__ import division, print_function\n",
    "from logbook import Logger, StreamHandler\n",
    "import sys\n",
    "StreamHandler(sys.stdout).push_application()\n",
    "log = Logger('Logbook')\n",
    "\n",
    "import utils; reload(utils)\n",
    "from utils import *\n",
    "import gc\n",
    "# from __future__ import division, print_function\n",
    "from theano.sandbox import cuda\n",
    "from vgg16bn import Vgg16BN\n",
    "from sklearn import metrics\n",
    "\n",
    "def accuracyfunc(y_act, y_pred):\n",
    "    return metrics.accuracy_score(np.argmax(y_act, axis=1), np.argmax(y_pred, axis=1))\n",
    "\n",
    "\n",
    "# Set Parameters and check files\n",
    "input_exists = True\n",
    "full = False\n",
    "log.info('Set Paramters')\n",
    "path = \"../data/fish/\"\n",
    "batch_size=64\n",
    "\n",
    "# Read in our VGG pretrained model\n",
    "log.info('Get VGG')\n",
    "model = vgg_ft_bn(8)\n",
    "\n",
    "# Create our VGG model\n",
    "log.info('Create VGG')\n",
    "vgg640 = Vgg16BN((360, 640)).model\n",
    "vgg640.pop()\n",
    "vgg640.input_shape, vgg640.output_shape\n",
    "vgg640.compile(Adam(), 'categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# get labels\n",
    "(val_classes, trn_classes, val_labels, trn_labels,\n",
    "    val_filenames, filenames, test_filenames) = get_classes(path)\n",
    "\n",
    "# Read in filenames\n",
    "log.info('Read filenames')\n",
    "raw_filenames = [f.split('/')[-1] for f in filenames]\n",
    "raw_test_filenames = [f.split('/')[-1] for f in test_filenames]\n",
    "raw_val_filenames = [f.split('/')[-1] for f in val_filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-02-07 21:49:00.531780] INFO: Logbook: Read in data\n"
     ]
    }
   ],
   "source": [
    "log.info('Read in data')\n",
    "if not input_exists:\n",
    "\n",
    "    batches = get_batches(path+'train', batch_size=batch_size)\n",
    "    val_batches = get_batches(path+'valid', batch_size=batch_size*2, shuffle=False)\n",
    "    (val_classes, trn_classes, val_labels, trn_labels, \n",
    "        val_filenames, filenames, test_filenames) = get_classes(path)\n",
    "    \n",
    "    # Fetch our large images \n",
    "    log.info('Fetch images')\n",
    "    trn = get_data(path+'train', (360,640))\n",
    "    val = get_data(path+'valid', (360,640))\n",
    "    test = get_data(path+'test', (360,640))\n",
    "    \n",
    "    # Precompute the output of the convolutional part of VGG\n",
    "    log.info('Get VGG output')\n",
    "    conv_val_feat = vgg640.predict(val, batch_size=32, verbose=1)\n",
    "    conv_trn_feat = vgg640.predict(trn, batch_size=32, verbose=1)\n",
    "    conv_test_feat = vgg640.predict(test, batch_size=32, verbose=1)\n",
    "    log.info('Write VGG output')\n",
    "    save_array(path+'results/conv_val_feat.dat', conv_val_feat)\n",
    "    save_array(path+'results/conv_trn_feat.dat', conv_trn_feat) \n",
    "    save_array(path+'results/conv_test_feat.dat', conv_test_feat)     \n",
    "\n",
    "    # For memory purposes delete out the original train and validation\n",
    "    log.info('Clear up memory')\n",
    "    del trn, val, test\n",
    "    gc.collect()\n",
    "\n",
    "conv_val_feat = load_array(path+'results/conv_val_feat.dat')\n",
    "conv_trn_feat = load_array(path+'results/conv_trn_feat.dat') \n",
    "conv_test_feat = load_array(path+'results/conv_test_feat.dat')\n",
    "\n",
    "if full:\n",
    "    conv_trn_feat = np.concatenate([conv_trn_feat, conv_val_feat])\n",
    "    trn_labels = np.concatenate([trn_labels, val_labels]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-02-07 22:23:12.302540] INFO: Logbook: Create and fit CNN\n",
      "[2017-02-07 22:23:12.304608] INFO: Logbook: Train round0\n",
      "th\n",
      "Train on 3277 samples, validate on 500 samples\n",
      "Epoch 1/2\n",
      "3277/3277 [==============================] - 16s - loss: 0.6367 - acc: 0.7952 - val_loss: 1.6308 - val_acc: 0.5600\n",
      "Epoch 2/2\n",
      "3277/3277 [==============================] - 16s - loss: 0.1219 - acc: 0.9710 - val_loss: 0.3252 - val_acc: 0.9200\n",
      "Train on 3277 samples, validate on 500 samples\n",
      "Epoch 1/6\n",
      "3277/3277 [==============================] - 16s - loss: 0.0302 - acc: 0.9924 - val_loss: 0.2971 - val_acc: 0.9240\n",
      "Epoch 2/6\n",
      "3277/3277 [==============================] - 16s - loss: 0.0258 - acc: 0.9936 - val_loss: 0.1600 - val_acc: 0.9620\n",
      "Epoch 3/6\n",
      "3277/3277 [==============================] - 16s - loss: 0.0048 - acc: 0.9994 - val_loss: 0.1730 - val_acc: 0.9640\n",
      "Epoch 4/6\n",
      "3277/3277 [==============================] - 16s - loss: 0.0016 - acc: 1.0000 - val_loss: 0.1488 - val_acc: 0.9700\n",
      "Epoch 5/6\n",
      "3277/3277 [==============================] - 16s - loss: 8.1792e-04 - acc: 1.0000 - val_loss: 0.1517 - val_acc: 0.9720\n",
      "Epoch 6/6\n",
      "3277/3277 [==============================] - 16s - loss: 5.6431e-04 - acc: 1.0000 - val_loss: 0.1540 - val_acc: 0.9720\n",
      "[2017-02-07 22:25:36.516116] INFO: Logbook: Output Prediction\n",
      "[2017-02-07 22:25:41.626614] INFO: Logbook: Bagged Validation Logloss 0.154\n",
      "[2017-02-07 22:25:41.627400] INFO: Logbook: Bagged Validation Accuracy 0.972\n",
      "[2017-02-07 22:25:41.628073] INFO: Logbook: Train round1\n",
      "th\n",
      "Train on 3277 samples, validate on 500 samples\n",
      "Epoch 1/2\n",
      "3277/3277 [==============================] - 16s - loss: 0.6346 - acc: 0.7995 - val_loss: 1.0425 - val_acc: 0.6840\n",
      "Epoch 2/2\n",
      "3277/3277 [==============================] - 16s - loss: 0.1266 - acc: 0.9649 - val_loss: 0.5060 - val_acc: 0.8140\n",
      "Train on 3277 samples, validate on 500 samples\n",
      "Epoch 1/6\n",
      "3277/3277 [==============================] - 16s - loss: 0.0537 - acc: 0.9893 - val_loss: 0.2087 - val_acc: 0.9560\n",
      "Epoch 2/6\n",
      "3277/3277 [==============================] - 16s - loss: 0.0101 - acc: 0.9979 - val_loss: 0.1648 - val_acc: 0.9660\n",
      "Epoch 3/6\n",
      "3277/3277 [==============================] - 16s - loss: 0.0038 - acc: 0.9994 - val_loss: 0.1435 - val_acc: 0.9740\n",
      "Epoch 4/6\n",
      "3277/3277 [==============================] - 16s - loss: 0.0035 - acc: 0.9994 - val_loss: 0.1385 - val_acc: 0.9760\n",
      "Epoch 5/6\n",
      "3277/3277 [==============================] - 16s - loss: 0.0017 - acc: 1.0000 - val_loss: 0.1412 - val_acc: 0.9760\n",
      "Epoch 6/6\n",
      "3277/3277 [==============================] - 16s - loss: 9.8545e-04 - acc: 0.9997 - val_loss: 0.1565 - val_acc: 0.9720\n",
      "[2017-02-07 22:28:05.743143] INFO: Logbook: Output Prediction\n",
      "[2017-02-07 22:28:11.849807] INFO: Logbook: Bagged Validation Logloss 0.143\n",
      "[2017-02-07 22:28:11.850677] INFO: Logbook: Bagged Validation Accuracy 0.976\n",
      "[2017-02-07 22:28:11.851266] INFO: Logbook: Train round2\n",
      "th\n",
      "Train on 3277 samples, validate on 500 samples\n",
      "Epoch 1/2\n",
      "3277/3277 [==============================] - 16s - loss: 0.6592 - acc: 0.7946 - val_loss: 1.1721 - val_acc: 0.6080\n",
      "Epoch 2/2\n",
      "3277/3277 [==============================] - 16s - loss: 0.1276 - acc: 0.9634 - val_loss: 0.4112 - val_acc: 0.8700\n",
      "Train on 3277 samples, validate on 500 samples\n",
      "Epoch 1/6\n",
      "3277/3277 [==============================] - 16s - loss: 0.0445 - acc: 0.9890 - val_loss: 0.2333 - val_acc: 0.9480\n",
      "Epoch 2/6\n",
      "3277/3277 [==============================] - 16s - loss: 0.0248 - acc: 0.9945 - val_loss: 0.1652 - val_acc: 0.9680\n",
      "Epoch 3/6\n",
      "3277/3277 [==============================] - 16s - loss: 0.0080 - acc: 0.9985 - val_loss: 0.1428 - val_acc: 0.9760\n",
      "Epoch 4/6\n",
      "3277/3277 [==============================] - 16s - loss: 0.0036 - acc: 0.9991 - val_loss: 0.1327 - val_acc: 0.9800\n",
      "Epoch 5/6\n",
      "3277/3277 [==============================] - 16s - loss: 8.3946e-04 - acc: 1.0000 - val_loss: 0.1365 - val_acc: 0.9760\n",
      "Epoch 6/6\n",
      "3277/3277 [==============================] - 16s - loss: 4.6231e-04 - acc: 1.0000 - val_loss: 0.1422 - val_acc: 0.9760\n",
      "[2017-02-07 22:30:35.505479] INFO: Logbook: Output Prediction\n",
      "[2017-02-07 22:30:40.614814] INFO: Logbook: Bagged Validation Logloss 0.133\n",
      "[2017-02-07 22:30:40.615690] INFO: Logbook: Bagged Validation Accuracy 0.978\n"
     ]
    }
   ],
   "source": [
    "# Our Convolutional Net Architecture\n",
    "log.info('Create and fit CNN')\n",
    "def get_lrg_layers():\n",
    "    return [\n",
    "        BatchNormalization(axis=1, input_shape=conv_layers[-1].output_shape[1:]),\n",
    "        Convolution2D(nf,3,3, activation='relu', border_mode='same'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D(),\n",
    "        Convolution2D(nf,3,3, activation='relu', border_mode='same'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D(),\n",
    "        Convolution2D(nf,3,3, activation='relu', border_mode='same'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D((1,2)),\n",
    "        Convolution2D(8,3,3, border_mode='same'),\n",
    "        Dropout(p),\n",
    "        GlobalAveragePooling2D(),\n",
    "        Activation('softmax')\n",
    "    ]\n",
    "\n",
    "# Set up the fully convolutional net (FCN); \n",
    "conv_layers,_ = split_at(vgg640, Convolution2D)\n",
    "nf=128; p=0. # No dropout\n",
    "\n",
    "lrg_model = []\n",
    "predsls = []\n",
    "pvalsls = []\n",
    "bags = 3\n",
    "\n",
    "for i in range(bags):\n",
    "    log.info('Train round' + str(i))\n",
    "    lrg_model.append(Sequential(get_lrg_layers()))\n",
    "    #if i == 0:\n",
    "\t#lrg_model[i].summary()\n",
    "    lrg_model[i].compile(Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    lrg_model[i].fit(conv_trn_feat, trn_labels, batch_size=batch_size, nb_epoch=2, \n",
    "                 validation_data=(conv_val_feat, val_labels))\n",
    "    lrg_model[i].optimizer.lr=1e-5\n",
    "    lrg_model[i].fit(conv_trn_feat, trn_labels, batch_size=batch_size, nb_epoch=6,\n",
    "                 validation_data=(conv_val_feat, val_labels))\n",
    "\n",
    "    # Make our prediction on the lrg_model layer\n",
    "    log.info('Output Prediction')\n",
    "    predsls.append(lrg_model[i].predict(conv_test_feat, batch_size=batch_size)) # or try 32 batch_size\n",
    "    pvalsls.append(lrg_model[i].predict(conv_val_feat, batch_size=batch_size))\n",
    "    val_score = \"%.3f\" % metrics.log_loss(val_labels, sum(pvalsls)/len(pvalsls))\n",
    "    acc_score = \"%.3f\" % accuracyfunc(val_labels, do_clip(sum(pvalsls)/len(pvalsls), .99))\n",
    "    log.info('Bagged Validation Logloss ' + str(val_score))\n",
    "    log.info('Bagged Validation Accuracy ' + str(acc_score))\n",
    "    # 10 bagged : 0.131\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.0000e+00,   0.0000e+00,   0.0000e+00, ...,   0.0000e+00,   0.0000e+00,   0.0000e+00],\n",
       "       [  1.0000e+00,   0.0000e+00,   0.0000e+00, ...,   0.0000e+00,   0.0000e+00,   0.0000e+00],\n",
       "       [  1.0000e+00,   0.0000e+00,   0.0000e+00, ...,   0.0000e+00,   0.0000e+00,   0.0000e+00],\n",
       "       ..., \n",
       "       [  1.8985e-01,   5.3936e-02,   4.4636e-03, ...,   5.1575e-01,   4.8795e-02,   2.4992e-02],\n",
       "       [  9.9704e-01,   1.6194e-04,   3.0691e-04, ...,   1.5662e-03,   3.8003e-05,   4.8699e-04],\n",
       "       [  8.3429e-03,   1.2262e-01,   4.4068e-03, ...,   7.5299e-02,   1.9163e-02,   5.6677e-01]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_pseu_feat = np.concatenate([conv_trn_feat, conv_test_feat])\n",
    "pseu_labels = np.concatenate([trn_labels, sum(predsls)/len(predsls)])\n",
    "pseu_labels[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-02-07 22:31:30.784769] INFO: Logbook: Train round0\n",
      "th\n",
      "Train on 4277 samples, validate on 500 samples\n",
      "Epoch 1/2\n",
      "4277/4277 [==============================] - 20s - loss: 0.6741 - acc: 0.7891 - val_loss: 1.3515 - val_acc: 0.6960\n",
      "Epoch 2/2\n",
      "4277/4277 [==============================] - 20s - loss: 0.2161 - acc: 0.9551 - val_loss: 0.4706 - val_acc: 0.8340\n",
      "Train on 4277 samples, validate on 500 samples\n",
      "Epoch 1/6\n",
      "4277/4277 [==============================] - 20s - loss: 0.1682 - acc: 0.9687 - val_loss: 0.1947 - val_acc: 0.9580\n",
      "Epoch 2/6\n",
      "4277/4277 [==============================] - 20s - loss: 0.1516 - acc: 0.9769 - val_loss: 0.2063 - val_acc: 0.9580\n",
      "Epoch 3/6\n",
      "4277/4277 [==============================] - 20s - loss: 0.1450 - acc: 0.9759 - val_loss: 0.1930 - val_acc: 0.9540\n",
      "Epoch 4/6\n",
      "4277/4277 [==============================] - 20s - loss: 0.1406 - acc: 0.9785 - val_loss: 0.1650 - val_acc: 0.9660\n",
      "Epoch 5/6\n",
      "4277/4277 [==============================] - 20s - loss: 0.1311 - acc: 0.9801 - val_loss: 0.1274 - val_acc: 0.9780\n",
      "Epoch 6/6\n",
      "4277/4277 [==============================] - 20s - loss: 0.1272 - acc: 0.9804 - val_loss: 0.1507 - val_acc: 0.9720\n",
      "[2017-02-07 22:34:25.958328] INFO: Logbook: Output Prediction\n",
      "[2017-02-07 22:34:30.837395] INFO: Logbook: Bagged Validation Logloss 0.151\n",
      "[2017-02-07 22:34:30.838291] INFO: Logbook: Bagged Validation Accuracy 0.972\n",
      "[2017-02-07 22:34:30.838922] INFO: Logbook: Train round1\n",
      "th\n",
      "Train on 4277 samples, validate on 500 samples\n",
      "Epoch 1/2\n",
      "4277/4277 [==============================] - 20s - loss: 0.6708 - acc: 0.8013 - val_loss: 1.0607 - val_acc: 0.7140\n",
      "Epoch 2/2\n",
      "4277/4277 [==============================] - 20s - loss: 0.2197 - acc: 0.9488 - val_loss: 0.3776 - val_acc: 0.8820\n",
      "Train on 4277 samples, validate on 500 samples\n",
      "Epoch 1/6\n",
      "4277/4277 [==============================] - 20s - loss: 0.1564 - acc: 0.9733 - val_loss: 0.2661 - val_acc: 0.9340\n",
      "Epoch 2/6\n",
      "4277/4277 [==============================] - 20s - loss: 0.1576 - acc: 0.9708 - val_loss: 0.1809 - val_acc: 0.9580\n",
      "Epoch 3/6\n",
      "4277/4277 [==============================] - 20s - loss: 0.1449 - acc: 0.9755 - val_loss: 0.2736 - val_acc: 0.9360\n",
      "Epoch 4/6\n",
      "4277/4277 [==============================] - 20s - loss: 0.1345 - acc: 0.9808 - val_loss: 0.1872 - val_acc: 0.9500\n",
      "Epoch 5/6\n",
      "4277/4277 [==============================] - 20s - loss: 0.1354 - acc: 0.9780 - val_loss: 0.1434 - val_acc: 0.9680\n",
      "Epoch 6/6\n",
      "4277/4277 [==============================] - 20s - loss: 0.1343 - acc: 0.9776 - val_loss: 0.1341 - val_acc: 0.9720\n",
      "[2017-02-07 22:37:25.290452] INFO: Logbook: Output Prediction\n",
      "[2017-02-07 22:37:30.144583] INFO: Logbook: Bagged Validation Logloss 0.132\n",
      "[2017-02-07 22:37:30.145420] INFO: Logbook: Bagged Validation Accuracy 0.980\n",
      "[2017-02-07 22:37:30.146063] INFO: Logbook: Train round2\n",
      "th\n",
      "Train on 4277 samples, validate on 500 samples\n",
      "Epoch 1/2\n",
      "4277/4277 [==============================] - 20s - loss: 0.6584 - acc: 0.7992 - val_loss: 1.0806 - val_acc: 0.6400\n",
      "Epoch 2/2\n",
      "4277/4277 [==============================] - 20s - loss: 0.2133 - acc: 0.9567 - val_loss: 0.5075 - val_acc: 0.8300\n",
      "Train on 4277 samples, validate on 500 samples\n",
      "Epoch 1/6\n",
      "4277/4277 [==============================] - 20s - loss: 0.1638 - acc: 0.9715 - val_loss: 0.1830 - val_acc: 0.9660\n",
      "Epoch 2/6\n",
      "4277/4277 [==============================] - 20s - loss: 0.1472 - acc: 0.9771 - val_loss: 0.1467 - val_acc: 0.9820\n",
      "Epoch 3/6\n",
      "4277/4277 [==============================] - 20s - loss: 0.1416 - acc: 0.9759 - val_loss: 0.1464 - val_acc: 0.9680\n",
      "Epoch 4/6\n",
      "4277/4277 [==============================] - 20s - loss: 0.1371 - acc: 0.9769 - val_loss: 0.1470 - val_acc: 0.9740\n",
      "Epoch 5/6\n",
      "4277/4277 [==============================] - 20s - loss: 0.1333 - acc: 0.9801 - val_loss: 0.1651 - val_acc: 0.9680\n",
      "Epoch 6/6\n",
      "4277/4277 [==============================] - 20s - loss: 0.1301 - acc: 0.9794 - val_loss: 0.1413 - val_acc: 0.9680\n",
      "[2017-02-07 22:40:25.182620] INFO: Logbook: Output Prediction\n",
      "[2017-02-07 22:40:30.043407] INFO: Logbook: Bagged Validation Logloss 0.131\n",
      "[2017-02-07 22:40:30.044237] INFO: Logbook: Bagged Validation Accuracy 0.978\n"
     ]
    }
   ],
   "source": [
    "lrg_model = []\n",
    "predsls = []\n",
    "pvalsls = []\n",
    "bags = 3\n",
    "\n",
    "for i in range(bags):\n",
    "    log.info('Train round' + str(i))\n",
    "    lrg_model.append(Sequential(get_lrg_layers()))\n",
    "    #if i == 0:\n",
    "\t#lrg_model[i].summary()\n",
    "    lrg_model[i].compile(Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    lrg_model[i].fit(conv_pseu_feat, pseu_labels, batch_size=batch_size, nb_epoch=2, \n",
    "                 validation_data=(conv_val_feat, val_labels))\n",
    "    lrg_model[i].optimizer.lr=1e-5\n",
    "    lrg_model[i].fit(conv_pseu_feat, pseu_labels, batch_size=batch_size, nb_epoch=6,\n",
    "                 validation_data=(conv_val_feat, val_labels))\n",
    "\n",
    "    # Make our prediction on the lrg_model layer\n",
    "    log.info('Output Prediction')\n",
    "    predsls.append(lrg_model[i].predict(conv_test_feat, batch_size=batch_size)) # or try 32 batch_size\n",
    "    pvalsls.append(lrg_model[i].predict(conv_val_feat, batch_size=batch_size))\n",
    "    val_score = \"%.3f\" % metrics.log_loss(val_labels, sum(pvalsls)/len(pvalsls))\n",
    "    acc_score = \"%.3f\" % accuracyfunc(val_labels, do_clip(sum(pvalsls)/len(pvalsls), .99))\n",
    "    log.info('Bagged Validation Logloss ' + str(val_score))\n",
    "    log.info('Bagged Validation Accuracy ' + str(acc_score))\n",
    "    # 10 bagged : 0.131"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-02-07 22:41:16.203912] INFO: Logbook: Done - files @ ../data/fish/results/subm_bb_conv_lrg0207pseu1.csv.gz\n"
     ]
    }
   ],
   "source": [
    "# metrics.log_loss(val_labels, do_clip(sum(pvalsls)/len(pvalsls), .9999))\n",
    "preds = sum(predsls)/len(predsls)\n",
    "subm = do_clip(preds,0.999)\n",
    "subm_name = path+'results/subm_bb_conv_lrg0207pseu1.csv.gz'\n",
    "pred_name = path+'results/pred_bb_conv_lrg0207pseu1.csv.gz'\n",
    "\n",
    "classes = ['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT']\n",
    "submission = pd.DataFrame(subm, columns=classes)\n",
    "submission.insert(0, 'image', raw_test_filenames)\n",
    "submission.to_csv(subm_name, index=False, compression='gzip')\n",
    "subm1 = pd.DataFrame(preds, columns=classes)\n",
    "subm1.insert(0, 'image', raw_test_filenames)\n",
    "subm1.to_csv(pred_name, index=False, compression='gzip')\n",
    "\n",
    "log.info('Done - files @ ' + subm_name)\n",
    "\n",
    "# Bag 6 Original scores \n",
    "#[2017-02-07 16:06:28.804177] INFO: Logbook: Bagged Validation Logloss 0.169\n",
    "#[2017-02-07 16:06:28.804338] INFO: Logbook: Bagged Validation Logloss 0.978"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='../data/fish/results/subm_bb_conv_lrg0206A.csv.gz' target='_blank'>../data/fish/results/subm_bb_conv_lrg0206A.csv.gz</a><br>"
      ],
      "text/plain": [
       "/home/ubuntu/fish/data/fish/results/subm_bb_conv_lrg0206A.csv.gz"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subm_name = '../data/fish/results/subm_bb_conv_lrg0206A.csv.gz'\n",
    "FileLink(subm_name)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
