{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "# Read in Libraries\n",
    "from __future__ import division, print_function\n",
    "from logbook import Logger, StreamHandler\n",
    "import sys\n",
    "StreamHandler(sys.stdout).push_application()\n",
    "log = Logger('Logbook')\n",
    "import shutil, csv, time\n",
    "timestr = time.strftime(\"%Y%m%d\")\n",
    "import ujson as json\n",
    "import utils; reload(utils)\n",
    "from utils import *\n",
    "import gc\n",
    "# from __future__ import division, print_function\n",
    "from theano.sandbox import cuda\n",
    "from vgg16bn import Vgg16BN\n",
    "from sklearn import metrics\n",
    "%matplotlib inline\n",
    "\n",
    "def accuracyfunc(y_act, y_pred):\n",
    "    return metrics.accuracy_score(np.argmax(y_act, axis=1), np.argmax(y_pred, axis=1))\n",
    "    \n",
    "def refresh_directory_structure(name, sub_dirs):\n",
    "    gdir = os.path.join(path, name)\n",
    "    if os.path.exists(gdir):\n",
    "        shutil.rmtree(gdir)\n",
    "    os.makedirs(gdir)\n",
    "    for sub_dir in sub_dirs:\n",
    "        os.makedirs(os.path.join(gdir, sub_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-03-05 20:53:26.496255] INFO: Logbook: Set Paramters\n"
     ]
    }
   ],
   "source": [
    "# Set Parameters and check files\n",
    "refresh_directories = False\n",
    "input_exists = True\n",
    "full = True\n",
    "log.info('Set Paramters')\n",
    "path = \"../data/fish/crop/\"\n",
    "batch_size=32\n",
    "clip = 0.99\n",
    "bags = 1\n",
    "load_size = (250,250) #(360, 640)\n",
    "aug_batches = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-03-05 20:53:26.508692] INFO: Logbook: Get VGG\n",
      "[2017-03-05 20:53:29.658692] INFO: Logbook: Create VGG\n",
      "Found 2685 images belonging to 8 classes.\n",
      "Found 622 images belonging to 8 classes.\n",
      "Found 666 images belonging to 1 classes.\n",
      "[2017-03-05 20:53:30.167627] INFO: Logbook: Read filenames\n"
     ]
    }
   ],
   "source": [
    "# Read in our VGG pretrained model\n",
    "log.info('Get VGG')\n",
    "model = vgg_ft_bn(8)\n",
    "\n",
    "# Create our VGG model\n",
    "log.info('Create VGG')\n",
    "vgg640 = Vgg16BN(load_size).model\n",
    "vgg640.pop()\n",
    "vgg640.input_shape, vgg640.output_shape\n",
    "vgg640.compile(Adam(), 'categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# get labels\n",
    "(val_classes, trn_classes, val_labels, trn_labels,\n",
    "    val_filenames, filenames, test_filenames) = get_classes(path)\n",
    "\n",
    "# Read in filenames\n",
    "log.info('Read filenames')\n",
    "raw_filenames = [f.split('/')[-1] for f in filenames]\n",
    "raw_test_filenames = [f.split('/')[-1] for f in test_filenames]\n",
    "raw_val_filenames = [f.split('/')[-1] for f in val_filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 622 images belonging to 8 classes.\n",
      "Found 2685 images belonging to 8 classes.\n",
      "Found 666 images belonging to 1 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2870"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_t = image.ImageDataGenerator(rotation_range=15, height_shift_range=0.05,# horizontal_flip=True,\n",
    "                                 # zoom_range=0.2,\n",
    "                shear_range=0.05, channel_shift_range=20, width_shift_range=0.05)\n",
    "da_val_batches = get_batches(path+'valid', gen_t, batch_size=batch_size, shuffle=False, target_size=load_size)\n",
    "da_trn_batches = get_batches(path+'train', gen_t, batch_size=batch_size, shuffle=False, target_size=load_size)\n",
    "da_tst_batches = get_batches(path+'test', gen_t, batch_size=batch_size, shuffle=False, target_size=load_size)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-03-05 20:53:30.380722] INFO: Logbook: Read in data\n"
     ]
    }
   ],
   "source": [
    "log.info('Read in data')\n",
    "if not input_exists:\n",
    "    \n",
    "    # Fetch our large images \n",
    "    # Precompute the output of the convolutional part of VGG\n",
    "    log.info('Fetch images')\n",
    "    log.info('Get VGG output')\n",
    "    log.info('Write VGG output')\n",
    "    \n",
    "    #log.info('Save Val Weights')\n",
    "    da_conv_val_feat = vgg640.predict_generator(da_val_batches, da_val_batches.nb_sample*aug_batches)\n",
    "    save_array(path+'../results/da_conv_val_feat.dat', da_conv_val_feat)\n",
    "    del da_conv_val_feat\n",
    "    gc.collect()\n",
    "    \n",
    "    #log.info('Save Trn Weights')\n",
    "    da_conv_trn_feat = vgg640.predict_generator(da_trn_batches, da_trn_batches.nb_sample*aug_batches)\n",
    "    save_array(path+'../results/da_conv_trn_feat.dat', da_conv_trn_feat)\n",
    "    del da_conv_trn_feat\n",
    "    gc.collect()\n",
    "    \n",
    "    val = get_data(path+'valid', load_size)\n",
    "    conv_val_feat = vgg640.predict(val, batch_size=16, verbose=1)\n",
    "    save_array(path+'../results/dano_conv_val_feat.dat', conv_val_feat)\n",
    "    del val, conv_val_feat\n",
    "    gc.collect()\n",
    "    \n",
    "    test = get_data(path+'test', load_size)\n",
    "    conv_test_feat = vgg640.predict(test, batch_size=16, verbose=1)\n",
    "    save_array(path+'../results/dano_conv_test_feat.dat', conv_test_feat)     \n",
    "    del test, conv_test_feat\n",
    "    gc.collect()\n",
    "    gc.collect()\n",
    "    \n",
    "    trn = get_data(path+'train', load_size)\n",
    "    conv_trn_feat = vgg640.predict(trn, batch_size=16, verbose=1)    \n",
    "    del trn\n",
    "    gc.collect()\n",
    "    save_array(path+'../results/dano_conv_trn_feat.dat', conv_trn_feat) \n",
    "    del conv_trn_feat\n",
    "    gc.collect()\n",
    "    \n",
    "    # For memory purposes delete out the original train and validation\n",
    "    log.info('Clear up memory')\n",
    "    #del trn, val, test\n",
    "    gc.collect()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's include the real training data as well in its non-augmented form.\n",
    "da_conv_trn_feat = load_array(path+'../results/da_conv_trn_feat.dat')\n",
    "dano_conv_trn_feat = load_array(path+'../results/dano_conv_trn_feat.dat')\n",
    "gc.collect()\n",
    "da_conv_trn_feat = np.concatenate([da_conv_trn_feat, dano_conv_trn_feat])\n",
    "del dano_conv_trn_feat \n",
    "gc.collect()\n",
    "\n",
    "# Validation set shouldonly be augmented for a full run\n",
    "da_conv_val_feat = load_array(path+'../results/dano_conv_val_feat.dat')\n",
    "if full:\n",
    "    dano_conv_val_feat = load_array(path+'../results/da_conv_val_feat.dat')\n",
    "    da_conv_val_feat = np.concatenate([da_conv_val_feat, dano_conv_val_feat])\n",
    "    del dano_conv_val_feat \n",
    "    gc.collect()\n",
    "\n",
    "conv_test_feat = load_array(path+'../results/dano_conv_test_feat.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Since we've now got a dataset 3x bigger than before, we'll need to copy our labels 6 times too.\n",
    "da_trn_labels = np.concatenate([trn_labels]*(aug_batches + 1))\n",
    "#da_trn_bbox = np.concatenate([trn_bbox]*(aug_batches + 1))\n",
    "\n",
    "# Validation set shouldonly be augmented for a full run\n",
    "if full:\n",
    "    da_val_labels = np.concatenate([val_labels]*(aug_batches + 1))\n",
    "    #da_val_bbox = np.concatenate([val_bbox]*(aug_batches + 1))\n",
    "else:\n",
    "    da_val_labels = val_labels\n",
    "    #da_val_bbox = val_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classes = ['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT']\n",
    "def fish_only(mat):\n",
    "    return np.delete(mat, 4, axis=1)\n",
    "\n",
    "trn_of_labels = fish_only(da_trn_labels)\n",
    "val_of_labels = fish_only(da_val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_of_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-03-05 20:53:44.587058] INFO: Logbook: Create and fit CNN\n"
     ]
    }
   ],
   "source": [
    "if full:\n",
    "    da_conv_trn_feat = np.concatenate([da_conv_trn_feat, da_conv_val_feat])\n",
    "    trn_of_labels = np.concatenate([trn_of_labels, val_of_labels]) \n",
    "    #trn_bbox = np.concatenate([trn_bbox, val_bbox])\n",
    "    \n",
    "# Our Convolutional Net Architecture\n",
    "log.info('Create and fit CNN')\n",
    "p=0.6\n",
    "# Set up the fully convolutional net (FCN); \n",
    "conv_layers,_ = split_at(vgg640, Convolution2D)\n",
    "nf=128; p=0. # No dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 15, 15)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_layers[-1].output_shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th\n",
      "Train on 19842 samples, validate on 3732 samples\n",
      "Epoch 1/5\n",
      "19842/19842 [==============================] - 24s - loss: 0.5481 - acc: 0.8257 - val_loss: 0.5312 - val_acc: 0.9633\n",
      "Epoch 2/5\n",
      "19842/19842 [==============================] - 24s - loss: 0.1657 - acc: 0.9562 - val_loss: 0.2640 - val_acc: 0.9890\n",
      "Epoch 3/5\n",
      "19842/19842 [==============================] - 24s - loss: 0.0740 - acc: 0.9830 - val_loss: 0.1701 - val_acc: 0.9976\n",
      "Epoch 4/5\n",
      "19842/19842 [==============================] - 24s - loss: 0.0353 - acc: 0.9942 - val_loss: 0.1005 - val_acc: 0.9995\n",
      "Epoch 5/5\n",
      "19842/19842 [==============================] - 24s - loss: 0.0202 - acc: 0.9970 - val_loss: 0.0762 - val_acc: 1.0000\n",
      "Train on 19842 samples, validate on 3732 samples\n",
      "Epoch 1/1\n",
      "19842/19842 [==============================] - 24s - loss: 0.0138 - acc: 0.9982 - val_loss: 0.0533 - val_acc: 1.0000\n",
      "[2017-03-05 20:57:59.143238] INFO: Logbook: Bagged Validation Logloss 0.053\n",
      "[2017-03-05 20:57:59.144024] INFO: Logbook: Bagged Validation Accuracy 1.000\n",
      "Train on 19842 samples, validate on 3732 samples\n",
      "Epoch 1/1\n",
      "19842/19842 [==============================] - 24s - loss: 0.0086 - acc: 0.9992 - val_loss: 0.0460 - val_acc: 1.0000\n",
      "[2017-03-05 20:58:26.019719] INFO: Logbook: Bagged Validation Logloss 0.050\n",
      "[2017-03-05 20:58:26.020502] INFO: Logbook: Bagged Validation Accuracy 1.000\n",
      "th\n",
      "Train on 19842 samples, validate on 3732 samples\n",
      "Epoch 1/5\n",
      "19842/19842 [==============================] - 24s - loss: 0.5489 - acc: 0.8225 - val_loss: 0.5766 - val_acc: 0.9630\n",
      "Epoch 2/5\n",
      "19842/19842 [==============================] - 24s - loss: 0.1671 - acc: 0.9577 - val_loss: 0.3110 - val_acc: 0.9914\n",
      "Epoch 3/5\n",
      "19842/19842 [==============================] - 24s - loss: 0.0772 - acc: 0.9825 - val_loss: 0.1875 - val_acc: 0.9960\n",
      "Epoch 4/5\n",
      "19842/19842 [==============================] - 24s - loss: 0.0409 - acc: 0.9931 - val_loss: 0.1284 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "19842/19842 [==============================] - 24s - loss: 0.0240 - acc: 0.9955 - val_loss: 0.0822 - val_acc: 1.0000\n",
      "Train on 19842 samples, validate on 3732 samples\n",
      "Epoch 1/1\n",
      "19842/19842 [==============================] - 24s - loss: 0.0168 - acc: 0.9977 - val_loss: 0.0712 - val_acc: 1.0000\n",
      "[2017-03-05 21:01:04.626777] INFO: Logbook: Bagged Validation Logloss 0.056\n",
      "[2017-03-05 21:01:04.627743] INFO: Logbook: Bagged Validation Accuracy 1.000\n",
      "Train on 19842 samples, validate on 3732 samples\n",
      "Epoch 1/1\n",
      "19842/19842 [==============================] - 24s - loss: 0.0122 - acc: 0.9983 - val_loss: 0.0563 - val_acc: 1.0000\n",
      "[2017-03-05 21:01:31.548030] INFO: Logbook: Bagged Validation Logloss 0.056\n",
      "[2017-03-05 21:01:31.548786] INFO: Logbook: Bagged Validation Accuracy 1.000\n",
      "th\n",
      "Train on 19842 samples, validate on 3732 samples\n",
      "Epoch 1/5\n",
      "19842/19842 [==============================] - 24s - loss: 0.5506 - acc: 0.8223 - val_loss: 0.5646 - val_acc: 0.9595\n",
      "Epoch 2/5\n",
      "19842/19842 [==============================] - 24s - loss: 0.1621 - acc: 0.9593 - val_loss: 0.2745 - val_acc: 0.9906\n",
      "Epoch 3/5\n",
      "19842/19842 [==============================] - 24s - loss: 0.0722 - acc: 0.9837 - val_loss: 0.1895 - val_acc: 0.9954\n",
      "Epoch 4/5\n",
      "19842/19842 [==============================] - 24s - loss: 0.0370 - acc: 0.9934 - val_loss: 0.1003 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "19842/19842 [==============================] - 24s - loss: 0.0205 - acc: 0.9974 - val_loss: 0.0783 - val_acc: 1.0000\n",
      "Train on 19842 samples, validate on 3732 samples\n",
      "Epoch 1/1\n",
      "19842/19842 [==============================] - 24s - loss: 0.0129 - acc: 0.9983 - val_loss: 0.0597 - val_acc: 1.0000\n",
      "[2017-03-05 21:04:10.431922] INFO: Logbook: Bagged Validation Logloss 0.057\n",
      "[2017-03-05 21:04:10.432747] INFO: Logbook: Bagged Validation Accuracy 1.000\n",
      "Train on 19842 samples, validate on 3732 samples\n",
      "Epoch 1/1\n",
      "19842/19842 [==============================] - 24s - loss: 0.0089 - acc: 0.9990 - val_loss: 0.0399 - val_acc: 1.0000\n",
      "[2017-03-05 21:04:37.408272] INFO: Logbook: Bagged Validation Logloss 0.054\n",
      "[2017-03-05 21:04:37.409030] INFO: Logbook: Bagged Validation Accuracy 1.000\n",
      "th\n",
      "Train on 19842 samples, validate on 3732 samples\n",
      "Epoch 1/5\n",
      "19842/19842 [==============================] - 24s - loss: 0.5367 - acc: 0.8282 - val_loss: 0.5400 - val_acc: 0.9748\n",
      "Epoch 2/5\n",
      "19842/19842 [==============================] - 24s - loss: 0.1599 - acc: 0.9582 - val_loss: 0.2984 - val_acc: 0.9906\n",
      "Epoch 3/5\n",
      "19842/19842 [==============================] - 24s - loss: 0.0724 - acc: 0.9847 - val_loss: 0.1678 - val_acc: 0.9989\n",
      "Epoch 4/5\n",
      "19842/19842 [==============================] - 24s - loss: 0.0401 - acc: 0.9927 - val_loss: 0.1148 - val_acc: 0.9997\n",
      "Epoch 5/5\n",
      "19842/19842 [==============================] - 24s - loss: 0.0253 - acc: 0.9953 - val_loss: 0.0855 - val_acc: 1.0000\n",
      "Train on 19842 samples, validate on 3732 samples\n",
      "Epoch 1/1\n",
      "19842/19842 [==============================] - 24s - loss: 0.0147 - acc: 0.9983 - val_loss: 0.0654 - val_acc: 1.0000\n",
      "[2017-03-05 21:07:16.097788] INFO: Logbook: Bagged Validation Logloss 0.055\n",
      "[2017-03-05 21:07:16.098566] INFO: Logbook: Bagged Validation Accuracy 1.000\n",
      "Train on 19842 samples, validate on 3732 samples\n",
      "Epoch 1/1\n",
      "19842/19842 [==============================] - 24s - loss: 0.0103 - acc: 0.9984 - val_loss: 0.0408 - val_acc: 1.0000\n",
      "[2017-03-05 21:07:42.984055] INFO: Logbook: Bagged Validation Logloss 0.053\n",
      "[2017-03-05 21:07:42.984820] INFO: Logbook: Bagged Validation Accuracy 1.000\n",
      "th\n",
      "Train on 19842 samples, validate on 3732 samples\n",
      "Epoch 1/5\n",
      "19842/19842 [==============================] - 24s - loss: 0.5558 - acc: 0.8254 - val_loss: 0.5797 - val_acc: 0.9695\n",
      "Epoch 2/5\n",
      "19842/19842 [==============================] - 24s - loss: 0.1770 - acc: 0.9535 - val_loss: 0.3171 - val_acc: 0.9877\n",
      "Epoch 3/5\n",
      "19842/19842 [==============================] - 24s - loss: 0.0776 - acc: 0.9822 - val_loss: 0.1705 - val_acc: 0.9984\n",
      "Epoch 4/5\n",
      "19842/19842 [==============================] - 24s - loss: 0.0381 - acc: 0.9937 - val_loss: 0.1067 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "19842/19842 [==============================] - 24s - loss: 0.0231 - acc: 0.9961 - val_loss: 0.0781 - val_acc: 1.0000\n",
      "Train on 19842 samples, validate on 3732 samples\n",
      "Epoch 1/1\n",
      "19842/19842 [==============================] - 24s - loss: 0.0143 - acc: 0.9982 - val_loss: 0.0672 - val_acc: 1.0000\n",
      "[2017-03-05 21:10:21.686542] INFO: Logbook: Bagged Validation Logloss 0.055\n",
      "[2017-03-05 21:10:21.687342] INFO: Logbook: Bagged Validation Accuracy 1.000\n",
      "Train on 19842 samples, validate on 3732 samples\n",
      "Epoch 1/1\n",
      "19842/19842 [==============================] - 24s - loss: 0.0113 - acc: 0.9981 - val_loss: 0.0517 - val_acc: 1.0000\n",
      "[2017-03-05 21:10:48.536768] INFO: Logbook: Bagged Validation Logloss 0.054\n",
      "[2017-03-05 21:10:48.537529] INFO: Logbook: Bagged Validation Accuracy 1.000\n",
      "th\n",
      "Train on 19842 samples, validate on 3732 samples\n",
      "Epoch 1/5\n",
      "19842/19842 [==============================] - 24s - loss: 0.5425 - acc: 0.8295 - val_loss: 0.5380 - val_acc: 0.9678\n",
      "Epoch 2/5\n",
      "19842/19842 [==============================] - 24s - loss: 0.1615 - acc: 0.9587 - val_loss: 0.3026 - val_acc: 0.9895\n",
      "Epoch 3/5\n",
      "19842/19842 [==============================] - 24s - loss: 0.0710 - acc: 0.9845 - val_loss: 0.2181 - val_acc: 0.9946\n",
      "Epoch 4/5\n",
      "19842/19842 [==============================] - 25s - loss: 0.0382 - acc: 0.9931 - val_loss: 0.1260 - val_acc: 0.9995\n",
      "Epoch 5/5\n",
      "19842/19842 [==============================] - 25s - loss: 0.0222 - acc: 0.9966 - val_loss: 0.0898 - val_acc: 0.9992\n",
      "Train on 19842 samples, validate on 3732 samples\n",
      "Epoch 1/1\n",
      "19842/19842 [==============================] - 25s - loss: 0.0140 - acc: 0.9984 - val_loss: 0.0662 - val_acc: 1.0000\n",
      "[2017-03-05 21:13:28.879683] INFO: Logbook: Bagged Validation Logloss 0.055\n",
      "[2017-03-05 21:13:28.880482] INFO: Logbook: Bagged Validation Accuracy 1.000\n",
      "Train on 19842 samples, validate on 3732 samples\n",
      "Epoch 1/1\n",
      "19842/19842 [==============================] - 24s - loss: 0.0107 - acc: 0.9981 - val_loss: 0.0477 - val_acc: 0.9997\n",
      "[2017-03-05 21:13:55.858622] INFO: Logbook: Bagged Validation Logloss 0.055\n",
      "[2017-03-05 21:13:55.859481] INFO: Logbook: Bagged Validation Accuracy 1.000\n",
      "th\n",
      "Train on 19842 samples, validate on 3732 samples\n",
      "Epoch 1/5\n",
      "19842/19842 [==============================] - 24s - loss: 0.5645 - acc: 0.8200 - val_loss: 0.5057 - val_acc: 0.9646\n",
      "Epoch 2/5\n",
      "19842/19842 [==============================] - 24s - loss: 0.1709 - acc: 0.9543 - val_loss: 0.2649 - val_acc: 0.9877\n",
      "Epoch 3/5\n",
      "19842/19842 [==============================] - 24s - loss: 0.0787 - acc: 0.9817 - val_loss: 0.1799 - val_acc: 0.9925\n",
      "Epoch 4/5\n",
      "19842/19842 [==============================] - 24s - loss: 0.0392 - acc: 0.9928 - val_loss: 0.1156 - val_acc: 0.9987\n",
      "Epoch 5/5\n",
      "19842/19842 [==============================] - 24s - loss: 0.0242 - acc: 0.9954 - val_loss: 0.0698 - val_acc: 0.9995\n",
      "Train on 19842 samples, validate on 3732 samples\n",
      "Epoch 1/1\n",
      "19842/19842 [==============================] - 24s - loss: 0.0144 - acc: 0.9979 - val_loss: 0.0532 - val_acc: 0.9997\n",
      "[2017-03-05 21:16:35.275619] INFO: Logbook: Bagged Validation Logloss 0.054\n",
      "[2017-03-05 21:16:35.276469] INFO: Logbook: Bagged Validation Accuracy 1.000\n",
      "Train on 19842 samples, validate on 3732 samples\n",
      "Epoch 1/1\n",
      "19842/19842 [==============================] - 24s - loss: 0.0097 - acc: 0.9989 - val_loss: 0.0372 - val_acc: 1.0000\n",
      "[2017-03-05 21:17:02.539164] INFO: Logbook: Bagged Validation Logloss 0.053\n",
      "[2017-03-05 21:17:02.540029] INFO: Logbook: Bagged Validation Accuracy 1.000\n",
      "th\n",
      "Train on 19842 samples, validate on 3732 samples\n",
      "Epoch 1/5\n",
      "19842/19842 [==============================] - 25s - loss: 0.5491 - acc: 0.8261 - val_loss: 0.5957 - val_acc: 0.9402\n",
      "Epoch 2/5\n",
      "19842/19842 [==============================] - 24s - loss: 0.1624 - acc: 0.9585 - val_loss: 0.3018 - val_acc: 0.9869\n",
      "Epoch 3/5\n",
      "19842/19842 [==============================] - 24s - loss: 0.0727 - acc: 0.9844 - val_loss: 0.2039 - val_acc: 0.9944\n",
      "Epoch 4/5\n",
      "19842/19842 [==============================] - 24s - loss: 0.0375 - acc: 0.9925 - val_loss: 0.1292 - val_acc: 0.9989\n",
      "Epoch 5/5\n",
      "19842/19842 [==============================] - 24s - loss: 0.0203 - acc: 0.9970 - val_loss: 0.0803 - val_acc: 1.0000\n",
      "Train on 19842 samples, validate on 3732 samples\n",
      "Epoch 1/1\n",
      "19842/19842 [==============================] - 25s - loss: 0.0147 - acc: 0.9977 - val_loss: 0.0651 - val_acc: 1.0000\n",
      "[2017-03-05 21:19:41.634435] INFO: Logbook: Bagged Validation Logloss 0.054\n",
      "[2017-03-05 21:19:41.635252] INFO: Logbook: Bagged Validation Accuracy 1.000\n",
      "Train on 19842 samples, validate on 3732 samples\n",
      "Epoch 1/1\n",
      "19842/19842 [==============================] - 25s - loss: 0.0121 - acc: 0.9982 - val_loss: 0.0485 - val_acc: 1.0000\n",
      "[2017-03-05 21:20:09.014552] INFO: Logbook: Bagged Validation Logloss 0.053\n",
      "[2017-03-05 21:20:09.015414] INFO: Logbook: Bagged Validation Accuracy 1.000\n",
      "th\n",
      "Train on 19842 samples, validate on 3732 samples\n",
      "Epoch 1/5\n",
      "19842/19842 [==============================] - 25s - loss: 0.5778 - acc: 0.8166 - val_loss: 0.5748 - val_acc: 0.9713\n",
      "Epoch 2/5\n",
      "19842/19842 [==============================] - 25s - loss: 0.1789 - acc: 0.9538 - val_loss: 0.2663 - val_acc: 0.9933\n",
      "Epoch 3/5\n",
      "19842/19842 [==============================] - 24s - loss: 0.0794 - acc: 0.9832 - val_loss: 0.1715 - val_acc: 0.9984\n",
      "Epoch 4/5\n",
      "19842/19842 [==============================] - 25s - loss: 0.0406 - acc: 0.9926 - val_loss: 0.1203 - val_acc: 0.9995\n",
      "Epoch 5/5\n",
      "19842/19842 [==============================] - 25s - loss: 0.0226 - acc: 0.9969 - val_loss: 0.0736 - val_acc: 1.0000\n",
      "Train on 19842 samples, validate on 3732 samples\n",
      "Epoch 1/1\n",
      "19842/19842 [==============================] - 25s - loss: 0.0156 - acc: 0.9975 - val_loss: 0.0568 - val_acc: 0.9997\n",
      "[2017-03-05 21:22:51.013492] INFO: Logbook: Bagged Validation Logloss 0.054\n",
      "[2017-03-05 21:22:51.014329] INFO: Logbook: Bagged Validation Accuracy 1.000\n",
      "Train on 19842 samples, validate on 3732 samples\n",
      "Epoch 1/1\n",
      "19842/19842 [==============================] - 25s - loss: 0.0125 - acc: 0.9977 - val_loss: 0.0557 - val_acc: 0.9997\n",
      "[2017-03-05 21:23:18.319777] INFO: Logbook: Bagged Validation Logloss 0.054\n",
      "[2017-03-05 21:23:18.320567] INFO: Logbook: Bagged Validation Accuracy 1.000\n",
      "th\n",
      "Train on 19842 samples, validate on 3732 samples\n",
      "Epoch 1/5\n",
      "19842/19842 [==============================] - 25s - loss: 0.5319 - acc: 0.8353 - val_loss: 0.5246 - val_acc: 0.9686\n",
      "Epoch 2/5\n",
      "19842/19842 [==============================] - 25s - loss: 0.1548 - acc: 0.9581 - val_loss: 0.2740 - val_acc: 0.9941\n",
      "Epoch 3/5\n",
      "19842/19842 [==============================] - 25s - loss: 0.0661 - acc: 0.9864 - val_loss: 0.1794 - val_acc: 0.9957\n",
      "Epoch 4/5\n",
      "19842/19842 [==============================] - 25s - loss: 0.0377 - acc: 0.9925 - val_loss: 0.1069 - val_acc: 0.9997\n",
      "Epoch 5/5\n",
      "19842/19842 [==============================] - 25s - loss: 0.0199 - acc: 0.9972 - val_loss: 0.0696 - val_acc: 1.0000\n",
      "Train on 19842 samples, validate on 3732 samples\n",
      "Epoch 1/1\n",
      "19842/19842 [==============================] - 25s - loss: 0.0126 - acc: 0.9980 - val_loss: 0.0560 - val_acc: 1.0000\n",
      "[2017-03-05 21:26:01.292191] INFO: Logbook: Bagged Validation Logloss 0.054\n",
      "[2017-03-05 21:26:01.293007] INFO: Logbook: Bagged Validation Accuracy 1.000\n",
      "Train on 19842 samples, validate on 3732 samples\n",
      "Epoch 1/1\n",
      "19842/19842 [==============================] - 25s - loss: 0.0088 - acc: 0.9987 - val_loss: 0.0492 - val_acc: 1.0000\n",
      "[2017-03-05 21:26:28.850746] INFO: Logbook: Bagged Validation Logloss 0.053\n",
      "[2017-03-05 21:26:28.851533] INFO: Logbook: Bagged Validation Accuracy 1.000\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "nf = 256\n",
    "p  = 0.5\n",
    "def create_model():\n",
    "    inp = Input(conv_layers[-1].output_shape[1:])\n",
    "    x =   Dropout(p)(inp)\n",
    "    x = BatchNormalization(axis=1)(x)\n",
    "    x = MaxPooling2D()(x)\n",
    "    x = ZeroPadding2D((1,1))(x)\n",
    "    x = Convolution2D(nf,3,3, activation='relu', border_mode='same')(x)\n",
    "    \n",
    "    x =   Dropout(p)(x)\n",
    "    x = BatchNormalization(axis=1)(x)\n",
    "    x1 =   MaxPooling2D()(x)\n",
    "    x1 =   Convolution2D(7,3,3, border_mode='same')(x1)\n",
    "    x1 =   GlobalAveragePooling2D()(x1)\n",
    "    x_class = Dense(7, activation='softmax', name='class')(x1)\n",
    "    \n",
    "    #x = Dropout(p/2)(x)\n",
    "    #x = Flatten()(x)\n",
    "    #x = Dense(1024, activation='relu')(x)\n",
    "    #x = BatchNormalization()(x)\n",
    "    #x_class = Dense(7, activation='softmax', name='class')(x1)\n",
    "    \n",
    "    return inp, x_class\n",
    "\n",
    "\n",
    "model, predsls, pvalsls = [], [], []\n",
    "\n",
    "for ii in range(10):\n",
    "    inp, x_class = create_model()\n",
    "    model.append(Model([inp], [x_class]))\n",
    "    model[ii].compile(Adam(lr=1e-4), loss=['categorical_crossentropy'], metrics=['accuracy']) # , decay=1e-6\n",
    "    #model[ii].summary()\n",
    "    model[ii].fit(da_conv_trn_feat, [trn_of_labels], batch_size=batch_size, nb_epoch=5, \n",
    "                 validation_data=(da_conv_val_feat, [val_of_labels]))\n",
    "    \n",
    "    model[ii].optimizer.lr = 1e-5\n",
    "    #model[ii].fit(da_conv_trn_feat, [trn_of_labels], batch_size=batch_size, nb_epoch=4, \n",
    "    #             validation_data=(da_conv_val_feat, [val_of_labels]))\n",
    "\n",
    "    count = 0\n",
    "    while count < 2:\n",
    "        model[ii].fit(da_conv_trn_feat, [trn_of_labels], batch_size=batch_size, nb_epoch=1, \n",
    "                     validation_data=(da_conv_val_feat, [val_of_labels]))\n",
    "        predsls.append(model[ii].predict(conv_test_feat, batch_size=batch_size)) # or try 32 batch_size\n",
    "        pvalsls.append(model[ii].predict(da_conv_val_feat, batch_size=batch_size))\n",
    "        val_score = \"%.3f\" % metrics.log_loss(val_of_labels, sum(pvalsls)/len(pvalsls))\n",
    "        acc_score = \"%.3f\" % accuracyfunc(val_of_labels, do_clip(sum(pvalsls)/len(pvalsls), clip))\n",
    "        log.info('Bagged Validation Logloss ' + str(val_score))\n",
    "        log.info('Bagged Validation Accuracy ' + str(acc_score))\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-03-05 21:26:53.292870] INFO: Logbook: Done - files @ ../data/fish/crop/../results/subm_full_crop_of_20170305A.csv\n"
     ]
    }
   ],
   "source": [
    "# metrics.log_loss(val_labels, do_clip(sum(pvalsls)/len(pvalsls), .9999))\n",
    "preds = sum(predsls)/len(predsls)\n",
    "subm = do_clip(preds, clip)\n",
    "\n",
    "if full:\n",
    "    subm_name = path+'../results/subm_full_crop_of_' + timestr + 'A.csv' #'.csv.gz'\n",
    "else:\n",
    "    subm_name = path+'../results/subm_part_crop_of_' + timestr + 'A.csv' #'.csv.gz'\n",
    "\n",
    "classes = ['ALB', 'BET', 'DOL', 'LAG', 'OTHER', 'SHARK', 'YFT']\n",
    "submission = pd.DataFrame(subm, columns=classes)\n",
    "submission.insert(0, 'image', raw_test_filenames)\n",
    "submission.to_csv(subm_name, index=False)#, compression='gzip')\n",
    "log.info('Done - files @ ' + subm_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='../data/fish/crop/../results/subm_full_crop_of_20170305A.csv' target='_blank'>../data/fish/crop/../results/subm_full_crop_of_20170305A.csv</a><br>"
      ],
      "text/plain": [
       "/home/ubuntu/fish/data/fish/results/subm_full_crop_of_20170305A.csv"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FileLink(subm_name)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
