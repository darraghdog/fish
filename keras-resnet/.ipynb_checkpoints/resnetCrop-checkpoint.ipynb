{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Adapted from keras example cifar10_cnn.py\n",
    "Train ResNet-18 on the CIFAR10 small images dataset.\n",
    "\n",
    "GPU run command with Theano backend (with TensorFlow, the GPU is automatically used):\n",
    "    THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python cifar10.py\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ReduceLROnPlateau, CSVLogger, EarlyStopping\n",
    "import numpy as np\n",
    "\n",
    "import resnet\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/home/ubuntu/fish/scripts')\n",
    "# Read in Libraries\n",
    "from __future__ import division, print_function\n",
    "from logbook import Logger, StreamHandler\n",
    "import sys\n",
    "StreamHandler(sys.stdout).push_application()\n",
    "log = Logger('Logbook')\n",
    "import shutil, csv, time\n",
    "timestr = time.strftime(\"%Y%m%d\")\n",
    "import ujson as json\n",
    "import utils; reload(utils)\n",
    "from utils import *\n",
    "import gc\n",
    "# from __future__ import division, print_function\n",
    "from theano.sandbox import cuda\n",
    "from vgg16bn import Vgg16BN\n",
    "from sklearn import metrics\n",
    "%matplotlib inline\n",
    "\n",
    "def accuracyfunc(y_act, y_pred):\n",
    "    return metrics.accuracy_score(np.argmax(y_act, axis=1), np.argmax(y_pred, axis=1))\n",
    "    \n",
    "def refresh_directory_structure(name, sub_dirs):\n",
    "    gdir = os.path.join(path, name)\n",
    "    if os.path.exists(gdir):\n",
    "        shutil.rmtree(gdir)\n",
    "    os.makedirs(gdir)\n",
    "    for sub_dir in sub_dirs:\n",
    "        os.makedirs(os.path.join(gdir, sub_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=np.sqrt(0.1), cooldown=0, patience=5, min_lr=0.5e-6)\n",
    "# lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=.5, cooldown=0, patience=5, min_lr=0.5e-7)\n",
    "early_stopper = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=10)\n",
    "csv_logger = CSVLogger('resnet18_cropped_fish.csv')\n",
    "\n",
    "batch_size = 32\n",
    "nb_classes = 8 # 10\n",
    "nb_epoch = 200\n",
    "data_augmentation = False\n",
    "path = \"../data/fish/relabel/\"\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 250, 250\n",
    "# The CIFAR10 images are RGB.\n",
    "img_channels = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2685 images belonging to 8 classes.\n",
      "Found 622 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "trn = get_data(path+'train', (img_rows, img_cols))\n",
    "val = get_data(path+'valid', (img_rows, img_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2685, 3, 200, 200)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2685 images belonging to 8 classes.\n",
      "Found 622 images belonging to 8 classes.\n",
      "Found 694 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# get labels\n",
    "(val_classes, trn_classes, val_labels, trn_labels,\n",
    "    val_filenames, filenames, test_filenames) = get_classes(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(622, 8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = trn.astype('float32')  # X_train.astype('float32')\n",
    "X_test =  val.astype('float32')  # X_test.astype('float32')\n",
    "\n",
    "# subtract mean and normalize\n",
    "mean_image = np.mean(X_train, axis=0)\n",
    "X_train -= mean_image\n",
    "X_test -= mean_image\n",
    "X_train /= 128.\n",
    "X_test /= 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train = trn_labels\n",
    "Y_test  = val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = resnet.ResnetBuilder.build_resnet_50((img_channels, img_rows, img_cols), nb_classes)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "#model.optimizer.lr = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using data augmentation.\n",
      "Train on 2685 samples, validate on 622 samples\n",
      "Epoch 1/200\n",
      "2685/2685 [==============================] - 127s - loss: 5.9349 - acc: 0.6212 - val_loss: 10.9173 - val_acc: 0.1849\n",
      "Epoch 2/200\n",
      "2685/2685 [==============================] - 129s - loss: 4.2113 - acc: 0.7538 - val_loss: 5.2584 - val_acc: 0.4373\n",
      "Epoch 3/200\n",
      "2685/2685 [==============================] - 128s - loss: 3.1163 - acc: 0.8425 - val_loss: 6.5307 - val_acc: 0.3039\n",
      "Epoch 4/200\n",
      "2685/2685 [==============================] - 126s - loss: 2.4197 - acc: 0.9065 - val_loss: 4.3489 - val_acc: 0.4453\n",
      "Epoch 5/200\n",
      "2685/2685 [==============================] - 127s - loss: 1.9874 - acc: 0.9330 - val_loss: 4.0868 - val_acc: 0.4839\n",
      "Epoch 6/200\n",
      "2685/2685 [==============================] - 126s - loss: 1.7022 - acc: 0.9512 - val_loss: 4.6472 - val_acc: 0.4421\n",
      "Epoch 7/200\n",
      "2685/2685 [==============================] - 126s - loss: 1.5157 - acc: 0.9471 - val_loss: 4.7289 - val_acc: 0.4502\n",
      "Epoch 8/200\n",
      "2685/2685 [==============================] - 126s - loss: 1.3536 - acc: 0.9583 - val_loss: 3.7427 - val_acc: 0.5080\n",
      "Epoch 9/200\n",
      "2685/2685 [==============================] - 126s - loss: 1.2463 - acc: 0.9657 - val_loss: 3.7041 - val_acc: 0.5466\n",
      "Epoch 10/200\n",
      "2685/2685 [==============================] - 124s - loss: 1.1396 - acc: 0.9713 - val_loss: 3.3331 - val_acc: 0.5113\n",
      "Epoch 11/200\n",
      "2685/2685 [==============================] - 125s - loss: 1.1194 - acc: 0.9497 - val_loss: 6.2116 - val_acc: 0.3103\n",
      "Epoch 12/200\n",
      "2685/2685 [==============================] - 125s - loss: 1.0255 - acc: 0.9698 - val_loss: 4.2386 - val_acc: 0.5434\n",
      "Epoch 13/200\n",
      "2685/2685 [==============================] - 127s - loss: 0.9506 - acc: 0.9765 - val_loss: 5.7180 - val_acc: 0.3971\n",
      "Epoch 14/200\n",
      "2685/2685 [==============================] - 127s - loss: 0.9163 - acc: 0.9683 - val_loss: 4.0247 - val_acc: 0.4968\n",
      "Epoch 15/200\n",
      "2685/2685 [==============================] - 128s - loss: 0.9381 - acc: 0.9553 - val_loss: 4.2428 - val_acc: 0.4871\n",
      "Epoch 16/200\n",
      "2685/2685 [==============================] - 128s - loss: 0.8440 - acc: 0.9758 - val_loss: 4.8189 - val_acc: 0.6077\n",
      "Epoch 17/200\n",
      "2685/2685 [==============================] - 126s - loss: 0.7589 - acc: 0.9926 - val_loss: 3.3982 - val_acc: 0.6029\n",
      "Epoch 18/200\n",
      "2685/2685 [==============================] - 127s - loss: 0.7167 - acc: 1.0000 - val_loss: 3.3009 - val_acc: 0.6190\n",
      "Epoch 19/200\n",
      "2685/2685 [==============================] - 127s - loss: 0.6943 - acc: 0.9993 - val_loss: 3.2423 - val_acc: 0.6318\n",
      "Epoch 20/200\n",
      "2685/2685 [==============================] - 126s - loss: 0.6719 - acc: 1.0000 - val_loss: 3.3269 - val_acc: 0.6174\n",
      "Epoch 21/200\n",
      "2685/2685 [==============================] - 126s - loss: 0.6522 - acc: 1.0000 - val_loss: 3.3235 - val_acc: 0.6158\n",
      "Epoch 22/200\n",
      "2685/2685 [==============================] - 127s - loss: 0.6326 - acc: 1.0000 - val_loss: 3.2714 - val_acc: 0.6174\n",
      "Epoch 23/200\n",
      "2685/2685 [==============================] - 128s - loss: 0.6142 - acc: 1.0000 - val_loss: 3.4048 - val_acc: 0.6174\n",
      "Epoch 24/200\n",
      "2685/2685 [==============================] - 127s - loss: 0.5967 - acc: 1.0000 - val_loss: 3.4254 - val_acc: 0.6190\n",
      "Epoch 25/200\n",
      "2685/2685 [==============================] - 128s - loss: 0.5794 - acc: 1.0000 - val_loss: 3.4518 - val_acc: 0.6125\n",
      "Epoch 26/200\n",
      "2685/2685 [==============================] - 126s - loss: 0.5680 - acc: 1.0000 - val_loss: 3.3883 - val_acc: 0.6109\n",
      "Epoch 27/200\n",
      "2685/2685 [==============================] - 127s - loss: 0.5638 - acc: 0.9996 - val_loss: 3.4673 - val_acc: 0.6013\n",
      "Epoch 28/200\n",
      "2685/2685 [==============================] - 127s - loss: 0.5587 - acc: 0.9996 - val_loss: 3.4588 - val_acc: 0.6141\n",
      "Epoch 29/200\n",
      "2685/2685 [==============================] - 128s - loss: 0.5523 - acc: 1.0000 - val_loss: 3.3318 - val_acc: 0.6125\n",
      "Epoch 30/200\n",
      "2685/2685 [==============================] - 128s - loss: 0.5468 - acc: 0.9996 - val_loss: 3.3910 - val_acc: 0.6254\n"
     ]
    }
   ],
   "source": [
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(X_train, Y_train,\n",
    "              batch_size=batch_size,\n",
    "              nb_epoch=nb_epoch,\n",
    "              verbose=1,\n",
    "              validation_data=(X_test, Y_test),\n",
    "              shuffle=True,\n",
    "              callbacks=[lr_reducer, early_stopper, csv_logger]) # \n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "    # Compute quantities required for featurewise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(X_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    model.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_size),\n",
    "                        samples_per_epoch=X_train.shape[0],\n",
    "                        validation_data=(X_test, Y_test),\n",
    "                        nb_epoch=nb_epoch, verbose=1, max_q_size=100,\n",
    "                        callbacks=[lr_reducer, early_stopper, csv_logger])\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
